<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[spring-boot整合kafka]]></title>
    <url>%2F2018%2F03%2F28%2Fkafka%2Fspringboot-kafka%2F</url>
    <content type="text"><![CDATA[生产者说明KafkaTemplate封装了一个生成器，并提供了方便的方法来发送数据到kafka主题。 提供了异步和同步方法，异步方法返回一个Future。其构造方法有： 123456789101112131415ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; sendDefault(V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; sendDefault(K key, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; sendDefault(int partition, K key, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; send(String topic, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; send(String topic, K key, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; send(String topic, int partition, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; send(String topic, int partition, K key, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; send(Message&lt;?&gt; message);]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git知识]]></title>
    <url>%2F2018%2F03%2F18%2Fgit%2F</url>
    <content type="text"><![CDATA[介绍git是一个开源的分布式版本控制系统，用以有效、高速的处理从很小到非常大的项目版本管理。git是个工具，在linux里面也就类似gcc这样的工具一样，是一个shell命令。 Git与GitHub区别Git是版本控制工具Github是一个平台，提供给用户创建git仓储空间，保存（托管）用户的一些数据文档或者代码等。 Git与CVS、SVN的区别Git是分布式版本控制系统，代码提交是在本地的（如此速度就快），当然生成补丁（patch）然后push到远程服务器上是需要联网的CVS、SVN是集中式版本控制系统，代码提交都是提交到远程服务器上CVS、SVN这样的集中式版本控制系统，它的完整代码仓库（代码仓库不仅仅只包含了代码，还包含各个历史版本的信息等）在中心服务器上，一旦这个中心服务器挂了，也就是完整的代码仓库挂了Git没有中心服务器的概念，每一个git客户端（git节点）都含有一个完整的代码仓库（前提是你之前从远程git仓库fetch过代码） 远程仓库、工作区、版本库和暂存区 远程仓库就是在github或者在gitlab上的代码。可以用git pull和git push来进行本地仓库和远程仓库的同步操作 工作区（Working Directory）从项目中取出某个版本的所有文件和目录，用以开始后续工作的叫做工作目录，也就是工作区 版本库（Repository）工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。 暂存区（Stage）暂存区就是版本库中的一个区域，具体参见上面的结构图 工作区、版本库、暂存区之间的关系使用git add把文件从工作区添加到版本库中的暂存区，git add命令可以多次用，或者使用git add file1 file2 …使用git commit提交代码，就是把暂存区的所有内容提交到当前分支需要提交的文件修改通通放到暂存区（可能有多次的git add），然后，一次性提交暂存区的所有修改到当前分支（git commit） 版本回退前提条件：已经执行git commit命令了，但是没有push到远程仓库，用以下命令可以回退其实这个回退就是将本地的HEAD指针移动到某个版本上而已，所以这个操作是非常快的。 回退到上一个版本在Git中，用HEAD表示当前版本，也就是最新的提交3628164…882e1e0，上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100版本号没必要写全，前几位就可以了，Git会自动去找。当然也不能只写前一两位，因为Git可能会找到多个版本号，就无法确定是哪一个了 12$ git reset --hard HEAD^HEAD is now at ea34578 add distributed 回退到未来的某个版本使用git log命令查看提交记录，找到需要回退版本的commit id如果在回退以后又想再次回到之前的版本，git reflog 可以查看所有分支的所有操作记录（包括commit和reset的操作），包括已经被删除的commit记录 12$ git reset --hard 3628164HEAD is now at 3628164 append GPL git log则不能查看已经删除了的commit记录 总结HEAD指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用命令git reset –hard commit_id穿梭前，用git log可以查看提交历史，以便确定要回退到哪个版本要重返未来，用git reflog查看命令历史，以便确定要回到未来的哪个版本 git reset –-soft：回退到某个版本，只回退了commit的信息，不会恢复到index file一级。如果还要提交，直接commit即可git reset -–hard：彻底回退到某个版本，本地的源码也会变为上一个版本的内容，撤销的commit中所包含的更改被冲掉 撤销修改含义命令git checkout – file意思就是，把file文件在工作区的修改全部撤销，这里有两种情况：一种是file自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；一种是file已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。总之，就是让这个文件回到最近一次git commit或git add时的状态。 场景场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout – file场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD file，就回到了场景1，第二步按场景1操作场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，版本回退一节，不过前提是没有推送到远程库 git stash场景:当你接到一个修复一个代号101的bug的任务时，你想创建一个分支issue-101来修复它，但是，当前正在dev上进行的工作还没有提交，Git还提供了一个stash功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作 git stash list查看暂存区的所有暂存修改记录 12$ git stash liststash@&#123;0&#125;: WIP on dev: 6224937 add merge git stash apply stash@{X}取出相应的暂存 git stash drop stash@{X}将记录列表中取出的对应暂存记录删除 git stash pop(推荐)取出最近一次暂存并删除记录列表中对应记录 分支管理 命令 含义 git branch 查看分支 git branch 创建分支 git checkout 切换分支 git checkout -b 创建+切换分支 git merge 合并某分支到当前分支 git branch -d 删除分支 Bug分支在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。 Feature分支添加一个新功能时，你肯定不希望因为一些实验性质的代码，把主分支搞乱了，所以，每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，最后，删除该feature分支。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis高级知识]]></title>
    <url>%2F2018%2F03%2F15%2Fredis%2Fredis%2F</url>
    <content type="text"><![CDATA[Redis 发布订阅Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。Redis 客户端可以订阅任意数量的频道。 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系：当有新消息通过 publish 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端： 实例以下实例演示了发布订阅是如何工作的。在我们实例中我们创建了订阅频道名为 redisChat: 123456redis 127.0.0.1:6379&gt; SUBSCRIBE redisChatReading messages... (press Ctrl-C to quit)1) "subscribe"2) "redisChat"3) (integer) 1 现在，我们先重新开启个 redis 客户端，然后在同一个频道 redisChat 发布两次消息，订阅者就能接收到消息。 123456789101112131415redis 127.0.0.1:6379&gt; publish redisChat "Redis is a great caching technique"(integer) 1redis 127.0.0.1:6379&gt; publish redisChat "Learn redis by runoob.com"(integer) 1# 订阅者的客户端会显示如下消息1) "message"2) "redisChat"3) "Redis is a great caching technique"1) "message"2) "redisChat"3) "Learn redis by runoob.com" 命令 命令 作用 PSUBSCRIBE pattern [pattern …] 订阅一个或多个符合给定模式的频道 PUBSUB subcommand [argument [argument …]] 查看订阅与发布系统状态 PUBLISH channel message 将信息发送到指定的频道 PUNSUBSCRIBE [pattern [pattern …]] 退订所有给定模式的频道 SUBSCRIBE channel [channel …] 订阅给定的一个或多个频道的信息 UNSUBSCRIBE [channel [channel …]] 指退订给定的频道 Redis 事务Redis 事务可以一次执行多个命令， 并且带有以下两个重要的保证： 批量操作在发送 EXEC 命令前被放入队列缓存。收到 EXEC 命令后进入事务执行，事务中任意命令执行失败，其余的命令依然被执行。在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中。 实例以下是一个事务的例子， 它先以 MULTI 开始一个事务， 然后将多个命令入队到事务中， 最后由 EXEC 命令触发事务， 一并执行事务中的所有命令： 12345678910111213141516171819202122redis 127.0.0.1:6379&gt; MULTIOKredis 127.0.0.1:6379&gt; SET book-name "Mastering C++ in 21 days"QUEUEDredis 127.0.0.1:6379&gt; GET book-nameQUEUEDredis 127.0.0.1:6379&gt; SADD tag "C++" "Programming" "Mastering Series"QUEUEDredis 127.0.0.1:6379&gt; SMEMBERS tagQUEUEDredis 127.0.0.1:6379&gt; EXEC1) OK2) "Mastering C++ in 21 days"3) (integer) 34) 1) "Mastering Series" 2) "C++" 3) "Programming" 单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 Redis 事务的执行并不是原子性的。事务可以理解为一个打包的批量执行脚本，但批量指令并非原子化的操作，中间某条指令的失败不会导致前面已做指令的回滚，也不会造成后续的指令不做。 命令 命令 作用 DISCARD 取消事务，放弃执行事务块内的所有命令 EXEC 执行所有事务块内的命令 MULTI 标记一个事务块的开始 UNWATCH 取消 WATCH 命令对所有 key 的监视 WATCH key [key …] 监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断 Redis 在事务失败时不进行回滚，而是继续执行余下的命令WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为 具体内容参见事物-官网文档 Redis 脚本Redis 脚本使用 Lua 解释器来执行脚本。 Reids 2.6 版本通过内嵌支持 Lua 环境。执行脚本的常用命令为 EVAL。 实例123456redis 127.0.0.1:6379&gt; EVAL "return &#123;KEYS[1],KEYS[2],ARGV[1],ARGV[2]&#125;" 2 key1 key2 first second1) "key1"2) "key2"3) "first"4) "second" 命令 命令 作用 EVAL script numkeys key [key …] arg [arg …] 执行 Lua 脚本 EVALSHA sha1 numkeys key [key …] arg [arg …] 执行 Lua 脚本 SCRIPT EXISTS script [script …] 查看指定的脚本是否已经被保存在缓存当中 SCRIPT FLUSH 从脚本缓存中移除所有脚本 SCRIPT KILL 杀死当前正在运行的 Lua 脚本 SCRIPT LOAD script 将脚本 script 添加到脚本缓存中，但并不立即执行这个脚本 Redis 数据备份与恢复Redis 提供了不同级别的持久化方式: RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储.AOF持久化方式记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF命令以redis协议追加保存每次写的操作到文件末尾.Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大. RDB优点RDB是一个非常紧凑的文件,它保存了某个时间点得数据集,非常适用于数据集的备份,比如你可以在每个小时报保存一下过去24小时内的数据,同时每天保存过去30天的数据,这样即使出了问题你也可以根据需求恢复到不同版本的数据集.RDB是一个紧凑的单一文件,很方便传送到另一个远端数据中心或者亚马逊的S3（可能加密），非常适用于灾难恢复.RDB在保存RDB文件时父进程唯一需要做的就是fork出一个子进程,接下来的工作全部由子进程来做，父进程不需要再做其他IO操作，所以RDB持久化方式可以最大化redis的性能.与AOF相比,在恢复大的数据集的时候，RDB方式会更快一些. 缺点如果你希望在redis意外停止工作（例如电源中断）的情况下丢失的数据最少的话，那么RDB不适合你.虽然你可以配置不同的save时间点(例如每隔5分钟并且对数据集有100个写的操作),是Redis要完整的保存整个数据集是一个比较繁重的工作,你通常会每隔5分钟或者更久做一次完整的保存,万一在Redis意外宕机,你可能会丢失几分钟的数据.RDB 需要经常fork子进程来保存数据集到硬盘上,当数据集比较大的时候,fork的过程是非常耗时的,可能会导致Redis在一些毫秒级内不能响应客户端的请求.如果数据集巨大并且CPU性能不是很好的情况下,这种情况会持续1秒,AOF也需要fork,但是你可以调节重写日志文件的频率来提高数据集的耐久度. AOF优点使用AOF 会让你的Redis更加耐久: 你可以使用不同的fsync策略：无fsync,每秒fsync,每次写的时候fsync.使用默认的每秒fsync策略,Redis的性能依然很好(fsync是由后台线程进行处理的,主线程会尽力处理客户端请求),一旦出现故障，你最多丢失1秒的数据.AOF文件是一个只进行追加的日志文件,所以不需要写入seek,即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令,你也也可使用redis-check-aof工具修复这些问题.Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 缺点对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。 命令 命令 作用 SAVE 创建当前数据库的备份,该命令将在 redis 安装目录中创建dump.rdb文件 BGSAVE 创建 redis 备份文件也可以使用命令 BGSAVE，该命令在后台执行 如果需要恢复数据，只需将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可 Redis 管道技术Redis是一种基于客户端-服务端模型以及请求/响应协议的TCP服务。这意味着通常情况下一个请求会遵循以下步骤： 客户端向服务端发送一个查询请求，并监听Socket返回，通常是以阻塞模式，等待服务端响应。服务端处理命令，并将结果返回给客户端。 更多知识详见官方文档]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis入门]]></title>
    <url>%2F2018%2F03%2F15%2Fredis%2Fredis-introduction%2F</url>
    <content type="text"><![CDATA[介绍Remote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统。Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Map), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。 数据类型String（字符串）string是redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。string类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。string类型是Redis最基本的数据类型，一个键最大能存储512MB。 实例1234redis 127.0.0.1:6379&gt; SET name "runoob"OKredis 127.0.0.1:6379&gt; GET name"runoob" 在以上实例中我们使用了 Redis 的 SET 和 GET 命令。键为 name，对应的值为 runoob。注意：一个键最大能存储512MB。 Hash（哈希）Redis hash 是一个键值(key=&gt;value)对集合。Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 实例123456redis&gt; HMSET myhash field1 "Hello" field2 "World""OK"redis&gt; HGET myhash field1"Hello"redis&gt; HGET myhash field2"World" 以上实例中 hash 数据类型存储了包含用户脚本信息的用户对象。 实例中我们使用了 Redis HMSET, HGETALL 命令，user:1 为键值。每个 hash 可以存储 2^32 -1 键值对（40多亿）。 List（列表）Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 实例1234567891011redis 127.0.0.1:6379&gt; lpush runoob redis(integer) 1redis 127.0.0.1:6379&gt; lpush runoob mongodb(integer) 2redis 127.0.0.1:6379&gt; lpush runoob rabitmq(integer) 3redis 127.0.0.1:6379&gt; lrange runoob 0 101) "rabitmq"2) "mongodb"3) "redis"redis 127.0.0.1:6379&gt; 列表最多可存储 2^32 - 1 元素 (4294967295, 每个列表可存储40多亿)。 Set（集合）Redis的Set是string类型的无序集合。集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 sadd 命令添加一个string元素到,key对应的set集合中，成功返回1,如果元素已经在集合中返回0,key对应的set不存在返回错误。 1sadd key member 实例12345678910111213redis 127.0.0.1:6379&gt; sadd runoob redis(integer) 1redis 127.0.0.1:6379&gt; sadd runoob mongodb(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 0redis 127.0.0.1:6379&gt; smembers runoob1) "rabitmq"2) "mongodb"3) "redis" 注意：以上实例中 rabitmq 添加了两次，但根据集合内元素的唯一性，第二次插入的元素将被忽略。集合中最大的成员数为 2^32 - 1(4294967295, 每个集合可存储40多亿个成员)。 zset(sorted set：有序集合)Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的,但分数(score)却可以重复。 zadd 命令添加元素到集合，元素在集合中存在则更新对应score 1zadd key score member 实例123456789101112redis 127.0.0.1:6379&gt; zadd runoob 0 redis(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 mongodb(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq(integer) 0redis 127.0.0.1:6379&gt; &gt; ZRANGEBYSCORE runoob 0 10001) "mongodb"2) "rabitmq"3) "redis"]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaptcha验证码介绍]]></title>
    <url>%2F2018%2F03%2F15%2Fcaptcha%2Fkaptcha%2F</url>
    <content type="text"><![CDATA[kaptcha介绍kaptcha 是一个很有用的验证码生成工具。有了它，你能够生成各种样式的验证码，由于它是可配置的。kaptcha工作的原理是调用com.google.code.kaptcha.servlet.KaptchaServlet，生成一个图片。同一时候将生成的验证码字符串放到 HttpSession中。 配置项 配置项 描述 可选值 默认值 kaptcha.border 图片边框 yes,no yes kaptcha.border.color 边框颜色 r,g,b(and optional alpha) 或者 white,black,blue black kaptcha.border.thickness 边框厚度 &gt;0 1 kaptcha.image.width 图片宽 &gt;0 200 kaptcha.image.height 图片高 &gt;0 50 kaptcha.producer.impl 图片实现类 com.google.code.kaptcha.impl.DefaultKaptcha kaptcha.textproducer.impl 文本实现类 com.google.code.kaptcha.text.impl.DefaultTextCreator kaptcha.textproducer.char.string 文本集合 abcde2345678gfynmnpwx kaptcha.textproducer.char.length 验证码长度 5 kaptcha.textproducer.font.names 字体 Arial, Courier kaptcha.textproducer.font.size 字体大小 40px kaptcha.textproducer.font.color 字体颜色 r,g,b 或者 white,black,blue black kaptcha.textproducer.char.space 文字间隔 2 kaptcha.noise.impl 干扰实现类 com.google.code.kaptcha.impl.DefaultNoise kaptcha.noise.color 干扰颜色 r,g,b 或者 white,black,blue black kaptcha.obscurificator.impl 图片样式 水纹com.google.code.kaptcha.impl.WaterRipple鱼眼com.google.code.kaptcha.impl.FishEyeGimpy阴影com.google.code.kaptcha.impl.ShadowGimpy com.google.code.kaptcha.impl.WaterRipple kaptcha.background.impl 背景实现类 com.google.code.kaptcha.impl.DefaultBackground kaptcha.background.clear.from 背景颜色渐变，开始颜色 lightGray kaptcha.background.clear.to 背景颜色渐变，结束颜色 white kaptcha.word.impl 文字渲染器 com.google.code.kaptcha.text.impl.DefaultWordRenderer kaptcha.session.key session key KAPTCHA_SESSION_KEY kaptcha.session.date session date KAPTCHA_SESSION_DATE]]></content>
      <categories>
        <category>kaptcha</category>
      </categories>
      <tags>
        <tag>kaptcha</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jcaptcha验证码介绍]]></title>
    <url>%2F2018%2F03%2F15%2Fcaptcha%2Fjcaptcha%2F</url>
    <content type="text"><![CDATA[jcaptcha组件介绍 组件 作用 FontGenerator 设置字体随机大小范围，字体名称 BackgroundGenerator 设置背景颜色，图片大小 TextPaster 设置单词的最小最大长度，设置字的颜色，设置单词在图像中的位置是固定还是随机 ImageDeformation、ImageFilter 非必须，指定为图片变形的类，可以用变形类和过滤器两种方式 WordToImage 将FontGenerator、BackgroundGenerator、TextPaster、ImageDeformation和ImageFilter组装到一起，属于中间环节的类，类似一个容器 WordGenerator 设置取词的范围，设置一定范围，或从本地词库读取 CaptchaFactory 将配置的类，组成工程类， 也属于包装类 结构图如下： 使用FontGeneratorFont类，用于设置字体的样式和字体随机的大小范围在com.octo.captcha.component.image.fontgenerator包中包含了几种Font类 Font 作用 示例 效果 RandomFontGenerator 设置随机出现的字体样式和随机的字体大小范围 RandomFontGenerator fonts = new RandomFontGenerator (new Integer(20), new Integer(30)); TwistedRandomFontGenerator 设置字体的随机大小范围，并对字体进行简单的扭曲变形，字体左右倾斜的效果 TwistedRandomFontGenerator fonts = new TwistedRandomFontGenerator(new Integer(20), new Integer(30)); TwistedAndShearedRandomFontGenerator TwistedRandomFontGenerator的子类，和 TwistedRandomFontGenerator类的构造器参数也相同，显示效果相近，采用的算法有所不同 DeformedRandomFontGenerator 构造器的参数和上两个类相同，采用的方式也类似 BackgroundGeneratorBackground设置背景颜色或背景图片、设置图片大小在com.octo.captcha.component.image.backgroundgenerator包中包含了的Background类 Background 作用 示例 效果 UniColorBackgroundGenerator 设置图片大小和以单一颜色为背景的图片背景，可以设置一个或多个备选颜色。如果想让颜色更加多变，则可以使用RandomRangeColorGenerator类，设置随机取得的RGB值，让RGB值分别的随机取得，则RGB值随机搭配的颜色将会更加多变 UniColorBackgroundGenerator background = new UniColorBackgroundGenerator(new Integer(200), new Integer(100), Color.BLUE); MultipleShapeBackgroundGenerator 设置一种多变的背景花纹 MultipleShapeBackgroundGenerator background = new MultipleShapeBackgroundGenerator (200,100,Color.RED,Color.BLUE,10,10,8,8,Color.WHITE,Color.YELLOW,3); GradientBackgroundGenerator 设置一种渐变颜色的背景，也可以设置多种颜色的渐变背景 GradientBackgroundGenerator background = new GradientBackgroundGenerator(200, 100, Color.BLACK, Color.WHITE); FunkyBackgroundGenerator 设置四种不同颜色混杂在一起的背景。如果创建过程中，给SingleColorGenerator类改为使用RandomListColorGenerator或其他类，也可以让四种颜色随机变化 SingleColorGenerator leftUpColor = new SingleColorGenerator(Color.RED); SingleColorGenerator leftDownColor = new SingleColorGenerator(Color.YELLOW); SingleColorGenerator rightUpColor = new SingleColorGenerator(Color.BLUE); SingleColorGenerator rightDownColor = new SingleColorGenerator(Color.GREEN); FunkyBackgroundGenerator background = new FunkyBackgroundGenerator( 200, 100, leftUpColor, leftDownColor, rightUpColor, rightDownColor, 0.5f); FileReaderRandomBackgroundGenerator 设置从指定目录读取图片作为生成图片的背景，目录中的图片，会在类构造期间就全部读取到内存中 FileReaderRandomBackgroundGenerator background = new FileReaderRandomBackgroundGenerator(200, 100, “E:¥¥testproject¥¥jcaptcha¥¥images¥¥backgrounds”); EllipseBackgroundGenerator 一种花样的背景，该类到目前为止没有更多的配置选项，只能设置图片大小，颜色和其他相关的值都不能设置 TextPasterTextPaster可以设置单词的最小和最大长度和设置字的颜色（前景颜色），设置单词在图像中的位置是固定还是随机 在com.octo.captcha.component.image.textpaster包中包含了的常用TextPaster类 TextPaster 作用 示例 效果 RandomTextPaster 最常用的TextPaster，可以设置生成单词的随机长度范围，和设置一种或多种前景颜色，生成的字符串在图像中的位置是随机的 RandomTextPaster textPaster = new RandomTextPaster(new Integer(5), new Integer(10), Color.BLACK);也可以设置多种随机前景颜色：Color[] colors = new Color[]{Color.RED,Color.YELLOW,Color.BLUE}; RandomListColorGenerator randomColors = new RandomListColorGenerator(colors); RandomTextPaster textPaster = new RandomTextPaster(new Integer(5), new Integer(10), randomColors); SimpleTextPaster 和RandomTextPaster用法和参数都相同，但是生成的字符串位置不变 NonLinearTextPaster 设置非直线的TextPaster，最后生成的文本成非直线的排列 NonLinearTextPaster textPaster = new NonLinearTextPaster (new Integer(5), new Integer(10), Color.BLACK); DoubleTextPaster 可以形成两行好像重影效果的文本，单词位置固定 DoubleTextPaster textPaster = new DoubleTextPaster(new Integer(5),new Integer(10), Color.BLACK); DoubleRandomTextPaster 和DoubleTextPaster类类似的效果，但是单词在图像中出现的位置和第二行字上下位置，是随机的 DecoratedRandomTextPaster 为生成的文本加上修饰，例如干扰线、干扰点 SingleColorGenerator lineColor = new SingleColorGenerator(Color.BLUE); TextDecorator decorator1 = new LineTextDecorator(new Integer(1), lineColor, AlphaComposite.SRC_OVER); TextDecorator[] decorators = new TextDecorator[] { decorator1 }; SingleColorGenerator color = new SingleColorGenerator(Color.RED); DecoratedRandomTextPaster textPaster = new DecoratedRandomTextPaster(new Integer(5), new Integer(10), color, decorators);LineTextDecorator的构造器的最后一个参数是干扰线的类型，在该参数上进行修改，也能取得不错的效果，例如将AlphaComposite.SRC_OVER改为AlphaComposite.CLEAR、AlphaComposite.SRC_IN、AlphaComposite.XORLineTextDecorator类是线型的装饰器，Captcha还提供了一种点状的装饰类：SingleColorGenerator dColor = new SingleColorGenerator(Color.BLUE); TextDecorator decorator2 = new BaffleTextDecorator(new Integer(1), dColor, AlphaComposite.SRC_OUT); TextDecorator[] decorators = new TextDecorator[] { decorator2 }; SingleColorGenerator color = new SingleColorGenerator(Color.RED); DecoratedRandomTextPaster textPaster = new DecoratedRandomTextPaster(new Integer(5), new Integer(10), color, decorators); Deformation和ImageFilter&nbsp;&nbsp;&nbsp;&nbsp;这两种类，都提供了对图片字体进行进一步变形的功能，Deformation类是Captcha框架中提供的变形类 和接口，ImageFilter是一个使用java提供图形图像方面服务的公司提供的通用的图形变形过滤器，可以实现的效果很多。&nbsp;&nbsp;&nbsp;&nbsp;Deformation和ImageFilter都分为3层，一种是在文字级别的，一种是在背景级别的，还有在整个图片级别的，用于在图片生成不同的阶段对图片进行变形。&nbsp;&nbsp;&nbsp;&nbsp;Deformation类，当前Captcha只提供了一种选择，就是PuzzleImageDeformation类，该类的效果是将 图片旋转，然后直接贴在原来图片的上面，所以效果不是很好，相信以后Captcha会提供更多的Deformation类，并且会不断完善，但是目前，不推荐使用。&nbsp;&nbsp;&nbsp;&nbsp;ImageFilter过滤器，有很多现有的资源可以供选择使用，是目前最好的选择。 12345678910111213141516171819202122232425// font RandomFontGenerator fonts = new RandomFontGenerator(new Integer(20), new Integer(20)); // background BackgroundGenerator background = new UniColorBackgroundGenerator(new Integer(400), new Integer(300), Color.white); // textPaster RandomTextPaster textPaster = new RandomTextPaster(new Integer(5), new Integer(5), Color.BLACK); // no deformation ImageDeformation noneDeformation = new ImageDeformationByFilters( new ImageFilter[] &#123;&#125;);// filter创建过滤器 ImageDeformation filters = new ImageDeformationByFilters( new ImageFilter[] &#123; new TwirlFilter() &#125;); DeformedComposedWordToImage cwti = new DeformedComposedWordToImage(fonts, background, textPaster, noneDeformation, filters, noneDeformation); // 设置随机取词范围RandomWordGenerator words = new RandomWordGenerator("ABCDEFGHIJKLMNOPQRSTUVWXYZ");GimpyFactory gimpy = new GimpyFactory(words, cwti); SimpleListImageCaptchaEngine engine = new SimpleListImageCaptchaEngine();engine.setFactories(new CaptchaFactory[] &#123; gimpy &#125;); FastHashMapCaptchaStore captchaStore = new FastHashMapCaptchaStore();// 创建Captcha服务 DefaultManageableImageCaptchaService defaultService = new DefaultManageableImageCaptchaService(captchaStore, engine, 180, 100000, 75000); return defaultService; &nbsp;&nbsp;&nbsp;&nbsp;上述的代码，是在创建一个Captcha服务器的过程中，将过滤器加入其中，其中使用的font、background、 和testPaster类，都使用了最简单的，没有变形效果的类，意在只查看过滤器对整个图片的映像。 更多过滤器效果，可以参见Java Image Filters]]></content>
      <categories>
        <category>jcaptcha</category>
      </categories>
      <tags>
        <tag>jcaptcha</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka知识探索]]></title>
    <url>%2F2018%2F03%2F11%2Fkafka%2Fkafka%2F</url>
    <content type="text"><![CDATA[环境搭建(Linux)1. Kafka下载 下载地址：http://kafka.apache.org/downloads wget http://apache.fayea.com/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz tar -xvf kafka_2.11-0.10.1.0.tgzcd kafka_2.11-0.10.1.0 2. Zookeeper安装Kafka需要Zookeeper的监控，所以先要安装Zookeeper，如何安装请传送至： hadoop、 zookeeper、 hbase、spark集群环境搭建 ，安装完成以后依次启动各个节点 3. 配置kafka broker集群 首先把Kafka解压后的目录复制到集群的各台服务器 然后修改各个服务器的配置文件：进入Kafka的config目录，修改server.properties12345678910# brokerid就是指各台服务器对应的id，所以各台服务器值不同broker.id=0# 端口号，无需改变port=9092# 当前服务器的IP，各台服务器值不同host.name=192.168.0.10# Zookeeper集群的ip和端口号zookeeper.connect=192.168.0.10:2181,192.168.0.11:2181,192.168.0.12:2181# 日志目录log.dirs=/home/www/kafka-logs 4. 启动Kafka 在每台服务器上进入Kafka目录，分别执行以下命令：1bin/kafka-server-start.sh config/server.properties &amp; 5. Kafka常用命令 5.1 新建topic1bin/kafka-topics.sh --create --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --replication-factor 2 --partitions 2 --topic test test有两个复制因子和两个分区 5.2 查看某个topic主题1bin/kafka-topics.sh --describe --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --topic test 其中第一行是所有分区的信息，下面的每一行对应一个分区Leader：负责某个分区所有读写操作的节点Replicas：复制因子节点Isr：存活节点 5.3 查看Kafka所有的主题1bin/kafka-topics.sh --list --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 5.4 终端发送消息1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 5.5 终端接收（消费）消息1bin/kafka-console-consumer.sh --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --bootstrap-server localhost:9092 --topic test --from-beginning 简介1. 基本术语消息在Kafka中，每一个消息由键、值和一个时间戳组成 主题和日志 Kafka集群存储同一类别的消息流称为主题 主题会有多个订阅者（0个1个或多个），当主题发布消息时，会向订阅者推送记录 针对每一个主题，Kafka集群维护了一个像下面这样的分区日志： 这些分区位于不同的服务器上，每一个分区可以看做是一个结构化的提交日志，每写入一条记录都会记录到其中一个分区并且分配一个唯一地标识其位置的数字称为偏移量offset Kafka集群会将发布的消息保存一段时间，不管是否被消费。例如，如果设置保存天数为2天，那么从消息发布起的两天之内，该消息一直可以被消费，但是超过两天后就会被丢弃以节省空间。其次，Kafka的数据持久化性能很好，所以长时间存储数据不是问题如下图所示，生产者每发布一条消息就会向分区log写入一条记录的offset，而消费者就是通过offset来读取对应的消息的，一般来说每读取一条消息，消费者对应要读取的offset就加1，例如最后一条读到offset=12，那么下条offset就为13.由于消费者通过offset来读取消息，所以可以重复读取已经读过的记录，或者跳过某些记录不读Kafka中采用分区的设计有几个目的。一是可以处理更多的消息，不受单台服务器的限制。Topic拥有多个分区意味着它可以不受限的处理更多的数据。第二，分区可以作为并行处理的单元 分布式Log的分区被分布到集群中的多个服务器上。每个服务器处理它分到的分区。 根据配置每个分区还可以复制到其它服务器作为备份容错每个分区有一个leader，零或多个follower。Leader处理此分区的所有的读写请求，而follower被动的复制数据。如果leader宕机，其它的一个follower会被推举为新的leader。 一台服务器可能同时是一个分区的leader，另一个分区的follower。 这样可以平衡负载，避免所有的请求都只让一台或者某几台服务器处理 生产者生产者往某个Topic上发布消息。生产者还可以选择将消息分配到Topic的哪个节点上。最简单的方式是轮询分配到各个分区以平衡负载，也可以根据某种算法依照权重选择分区 消费者Kafka有一个消费者组的概念，生产者把消息发到的是消费者组，在消费者组里面可以有很多个消费者实例，如下图所示： Kafka集群有两台服务器，四个分区，此外有两个消费者组A和B，消费者组A具有2个消费者实例C1-2，消费者B具有4个消费者实例C3-6那么Kafka发送消息的过程是怎样的呢？例如此时我们创建了一个主题test，有两个分区，分别是Server1的P0和Server2的P1，假设此时我们通过test发布了一条消息，那么这条消息是发到P0还是P1呢，或者是都发呢？答案是只会发到P0或P1其中之一，也就是消息只会发给其中的一个分区分区接收到消息后会记录在分区日志中，记录的方式我们讲过了，就是通过offset，正因为有这个偏移量的存在，所以一个分区内的消息是有先后顺序的，即offset大的消息比offset小的消息后到。但是注意，由于消息随机发往主题的任意一个分区，因此虽然同一个分区的消息有先后顺序，但是不同分区之间的消息就没有先后顺序了，那么如果我们要求消费者顺序消费主题发的消息那该怎么办呢，此时只要在创建主题的时候只提供一个分区即可讲完了主题发消息，接下来就该消费者消费消息了，假设上面test的消息发给了分区P0，此时从图中可以看到，有两个消费者组，那么P0将会把消息发到哪个消费者组呢？从图中可以看到，P0把消息既发给了消费者组A也发给了B，但是A中消息仅被C1消费，B中消息仅被C3消费。这就是我们要讲的，主题发出的消息会发往所有的消费者组，而每一个消费者组下面可以有很多消费者实例，这条消息只会被他们中的一个消费掉 2. 核心APIKafka具有4个核心API： Producer API：用于向Kafka主题发送消息。 Consumer API：用于从订阅主题接收消息并且处理这些消息。 Streams API：作为一个流处理器，用于从一个或者多个主题中消费消息流然后为其他主题生产消息流，高效地将输入流转换为输出流。 Connector API：用于构建和运行将Kafka主题和已有应用或者数据系统连接起来的可复用的生产者或消费者。例如一个主题到一个关系型数据库的连接能够捕获表的任意变化。 3. 应用场景Kafka用作消息系统Kafka流的概念与传统企业消息系统有什么异同？传统消息系统有两个模型：队列和发布-订阅系统。在队列模式中，每条服务器的消息会被消费者池中的一个所读取；而发布-订阅系统中消息会广播给所有的消费者。这两种模式各有优劣。队列模式的优势是可以将消息数据让多个消费者处理以实现程序的可扩展，然而这就导致其没有多个订阅者，只能用于一个进程。发布-订阅模式的好处在于数据可以被多个进程消费使用，但是却无法使单一程序扩展性能Kafka中消费者组的概念同时涵盖了这两方面。对应于队列的概念，Kafka中每个消费者组中有多个消费者实例可以接收消息；对应于发布-订阅模式，Kafka中可以指定多个消费者组来订阅消息相对传统消息系统，Kafka可以提供更强的顺序保证 Kafka用作存储系统任何发布消息与消费消息解耦的消息队列其实都可以看做是用来存放发布的消息的存储系统，而Kafka是一个非常高效的存储系统写入Kafka的数据会被存入磁盘并且复制到集群中以容错。Kafka允许生产者等待数据完全复制并且确保持久化到磁盘的确认应答Kafka使用的磁盘结构扩容性能很好——不管服务器上有50KB还是50TB，Kafka的表现都是一样的由于能够精致的存储并且供客户端程序进行读操作，你可以把Kafka看做是一个用于高性能、低延迟的存储提交日志、复制及传播的分布式文件系统 Kafka的流处理仅仅读、写、存储流数据是不够的，Kafka的目的是实现实时流处理。在Kafka中一个流处理器的处理流程是首先持续性的从输入主题中获取数据流，然后对其进行一些处理，再持续性地向输出主题中生产数据流。例如一个销售商应用，接收销售和发货量的输入流，输出新订单和调整后价格的输出流可以直接使用producer和consumer API进行简单的处理。对于复杂的转换，Kafka提供了更强大的Streams API。可构建聚合计算或连接流到一起的复杂应用程序流处理有助于解决这类应用面临的硬性问题：处理无序数据、代码更改的再处理、执行状态计算等Streams API所依托的都是Kafka的核心内容：使用producer和consumer API作为输入，使用Kafka作为状态存储，在流处理实例上使用相同的组机制来实现容错 使用消费者自动提交使用如下api自动提交： 1properties.put("enable.auto.commit", "false"); 消费者手动提交每个消费者和对应的patition建立对应的流来读取kafka上面的数据，如果comsumer得到数据，那么kafka就会自动去维护该comsumer的offset，例如在获取到kafka的消息后正准备入库（未入库），但是消费者挂了，那么如果让kafka自动去维护offset，它就会认为这条数据已经被消费了，那么会造成数据丢失。但是kafka可以让你自己去手动提交，如果在上面的场景中，那么需要我们手动commit，如果comsumer挂了 那么程序就不会执行commit这样的话 其他同group的消费者又可以消费这条数据，保证数据不丢，先要做如下设置： 12//设置不自动提交，自己手动更新offsetproperties.put("enable.auto.commit", "false"); 使用如下api提交： 1consumer.commitSync();]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK环境搭建]]></title>
    <url>%2F2017%2F08%2F04%2FELK%2F</url>
    <content type="text"><![CDATA[一、配置系统： Windows 8.1 elasticsearch：5.5.1 logstash：2.0.0 kibana：5.5.1 注：由于实验性搭建，选择windows系统，但选择Linux系统效果更佳 二、部署方案1.ELK+Redis2.ELK+Kafka 注：本次搭建选用第一种方案 三、安装 前提:下载nssm 1. Elasticsearch下载： download2. logstash下载： download3. kibana下载： download注册为windows服务(a) 将下载的nssm.exe分别拷贝到Elasticsearch、logstash和kibana解压后的bin目录下，然后CMD进入bin执行nssm install 服务名,例如Elasticsearch 的执行nssm install elasticsearch-service..(b) 分析选择path为各压缩包的bin目录下的elasticsearch.bat、logstash.bat和kibana.bat(c) Details选项卡设置显示名为Windows名(d) 最后选择Install service四、部署1. 创建Maven项目elk-log(可另外取名)，pom文件为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.suncj&lt;/groupId&gt; &lt;artifactId&gt;elk-log&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;elk-log&lt;/name&gt; &lt;description&gt;elk日志生成项目&lt;/description&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;4.2.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty.aggregate&lt;/groupId&gt; &lt;artifactId&gt;jetty-all&lt;/artifactId&gt; &lt;version&gt;8.1.19.v20160209&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;4.9&lt;/version&gt; &lt;/dependency&gt; &lt;!--实现slf4j接口并整合 --&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2. 配置logback,logback.xml文件为：1234567891011121314151617181920212223242526&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration debug="false"&gt; &lt;appender name="console" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;!-- 格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符 --&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %c&#123;1&#125;.%M:%L - %m%n &lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="stash" class="net.logstash.logback.appender.LogstashTcpSocketAppender"&gt; &lt;destination&gt;127.0.0.1:9250&lt;/destination&gt; &lt;encoder charset="UTF-8" class="net.logstash.logback.encoder.LogstashEncoder" /&gt; &lt;/appender&gt; &lt;logger name="com.suncj" level="INFO" /&gt; &lt;root level="INFO"&gt; &lt;appender-ref ref="console" /&gt; &lt;appender-ref ref="stash" /&gt; &lt;/root&gt;&lt;/configuration&gt; 3.设置项目定时任务(打日志)定时任务类LogProducer:12345678910111213141516171819202122232425package com.suncj.elk;import java.util.Random;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * 日志生成器&lt;br&gt; * 版权：Copyright (c) 2015-2016&lt;br&gt; * 创建日期：2017年8月5日&lt;br&gt; */public class LogProducer &#123; private static final Logger log = LoggerFactory.getLogger(LogProducer.class); private Random rand = new Random(); private static int logId = 0; public void produce() &#123; log.info("log_id: &#123;&#125; , content:&#123;&#125;", logId, String.format("I am %s", logId + rand.nextInt(100000))); logId++; &#125;&#125; 项目启动类:123456789101112131415161718192021222324package com.suncj.elk;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Application &#123; private static Logger logger = LoggerFactory.getLogger(Application.class); public static ApplicationContext appContext; public static void main(String[] args) &#123; try &#123; logger.info("准备加载程序"); appContext = new ClassPathXmlApplicationContext("app-*.xml"); logger.info("加载完成"); &#125; catch (Exception e) &#123; logger.error("主程序出错:", e); &#125; &#125;&#125; 其他配置文件：app-task.xml12345678910111213141516&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:util="http://www.springframework.org/schema/util" xmlns:task="http://www.springframework.org/schema/task" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"&gt; &lt;bean id="logProducer" class="com.suncj.elk.LogProducer"&gt;&lt;/bean&gt; &lt;task:scheduled-tasks&gt; &lt;task:scheduled ref="logProducer" method="produce" cron="0/5 * * * * *" /&gt; &lt;/task:scheduled-tasks&gt;&lt;/beans&gt; 2. logstash配置(a) run_es.bat，run_redis.bat1logstash.bat agent -f logstash_es.conf (b) logstash_redis.conf1234567891011121314151617input &#123; tcp &#123; host =&gt; "127.0.0.1" port =&gt; 9250 mode =&gt; "server" codec =&gt; json_lines &#125;&#125;output &#123; redis &#123; host =&gt; "127.0.0.1" port =&gt; 6379 db =&gt; 1 data_type =&gt; "list" key =&gt; "log:es" &#125;&#125; (c) logstash_es.conf12345678910111213141516171819input &#123; redis &#123; data_type =&gt; "list" key =&gt; "log:es" host =&gt; "127.0.0.1" db =&gt; 1 port =&gt; 6379 &#125;&#125;output &#123; stdout&#123; codec =&gt; rubydebug &#125; elasticsearch &#123; hosts =&gt; ["127.0.0.1:9200"] index =&gt; "log-es-%&#123;+YYYY.MM.dd&#125;" flush_size =&gt; 1000 &#125;&#125; 注: logstash注册为windows服务时需要创建两个bat文件，一个用于项目日志存储到redis；另外一个用户读取redis,输出到elasticsearch，因此需要注册两个服务名不同的windows服务 参考资料https://kibana.logstash.es/content/kibana/index.html http://blog.csdn.net/tulizi/article/details/52972824 http://udn.yyuap.com/doc/logstash-best-practice-cn/input/redis.html https://www.elastic.co/guide/en/logstash/current/codec-plugins.html]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-Shiro介绍及其使用]]></title>
    <url>%2F2017%2F08%2F01%2Fspring%2Fspring-shiro%2F</url>
    <content type="text"><![CDATA[What is Apache Shiro?Apache Shiro是一个功能强大、灵活的，开源的安全框架。它可以干净利落地处理身份验证、授权、企业会话管理和加密。 Apache Shiro的首要目标是易于使用和理解。安全通常很复杂，甚至让人感到很痛苦，但是Shiro却不是这样子的。一个好的安全框架应该屏蔽复杂性，向外暴露简单、直观的API，来简化开发人员实现应用程序安全所花费的时间和精力。Shiro能做什么呢？ 验证用户身份 用户访问权限控制，比如：1、判断用户是否分配了一定的安全角色。2、判断用户是否被授予完成某个操作的权限 在非 web 或 EJB 容器的环境下可以任意使用Session API 可以响应认证、访问控制，或者 Session 生命周期中发生的事件 可将一个或以上用户安全数据源数据组合成一个复合的用户 “view”(视图) 支持单点登录(SSO)功能 支持提供“Remember Me”服务，获取用户关联信息而无需登录… 等等——都集成到一个有凝聚力的易于使用的API。 Shiro 致力在所有应用环境下实现上述功能，小到命令行应用程序，大到企业应用中，而且不需要借助第三方框架、容器、应用服务器等。当然 Shiro 的目的是尽量的融入到这样的应用环境中去，但也可以在它们之外的任何环境下开箱即用。 Apache Shiro Features 特性Apache Shiro是一个全面的、蕴含丰富功能的安全框架。下图为描述Shiro功能的框架图： Authentication（认证）, Authorization（授权）, Session Management（会话管理）, Cryptography（加密）被 Shiro 框架的开发团队称之为应用安全的四大基石。那么就让我们来看看它们吧： Authentication（认证）：用户身份识别，通常被称为用户“登录” Authorization（授权）：访问控制。比如某个用户是否具有某个操作的使用权限。 Session Management（会话管理）：特定于用户的会话管理,甚至在非web 或 EJB 应用程序。 Cryptography（加密）：在对数据源使用加密算法加密的同时，保证易于使用。 还有其他的功能来支持和加强这些不同应用环境下安全领域的关注点。特别是对以下的功能支持： Web支持：Shiro 提供的 web 支持 api ，可以很轻松的保护 web 应用程序的安全。 缓存：缓存是 Apache Shiro 保证安全操作快速、高效的重要手段。 并发：Apache Shiro 支持多线程应用程序的并发特性。 测试：支持单元测试和集成测试，确保代码和预想的一样安全。 “Run As”：这个功能允许用户假设另一个用户的身份(在许可的前提下)。 “Remember Me”：跨 session 记录用户的身份，只有在强制需要时才需要登录。 注意： Shiro不会去维护用户、维护权限，这些需要我们自己去设计/提供，然后通过相应的接口注入给Shiro High-Level Overview 高级概述在概念层，Shiro 架构包含三个主要的理念：Subject,SecurityManager和 Realm。下面的图展示了这些组件如何相互作用，我们将在下面依次对其进行描述。 Subject：当前用户，Subject 可以是一个人，但也可以是第三方服务、守护进程帐户、时钟守护任务或者其它–当前和软件交互的任何事件。 SecurityManager：管理所有Subject，SecurityManager 是 Shiro 架构的核心，配合内部安全组件共同组成安全伞。 Realms：用于进行权限信息的验证，我们自己实现。Realm 本质上是一个特定的安全 DAO：它封装与数据源连接的细节，得到Shiro 所需的相关的数据。在配置 Shiro 的时候，你必须指定至少一个Realm 来实现认证（authentication）和/或授权（authorization）。 我们需要实现Realms的Authentication 和 Authorization。其中 Authentication 是用来验证用户身份，Authorization 是授权访问控制，用于对用户进行的操作授权，证明该用户是否允许进行当前操作，如访问某个链接，某个资源文件等。 快速上手pom.xml1234567891011121314151617181920212223242526272829&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sourceforge.nekohtml&lt;/groupId&gt; &lt;artifactId&gt;nekohtml&lt;/artifactId&gt; &lt;version&gt;1.9.22&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Shiro 配置1234567891011121314151617181920212223242526272829303132333435363738394041@Configurationpublic class ShiroConfig &#123; @Bean public ShiroFilterFactoryBean shirFilter(SecurityManager securityManager) &#123; System.out.println("ShiroConfiguration.shirFilter()"); ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); shiroFilterFactoryBean.setSecurityManager(securityManager); //拦截器. Map&lt;String,String&gt; filterChainDefinitionMap = new LinkedHashMap&lt;String,String&gt;(); // 配置不会被拦截的链接 顺序判断 filterChainDefinitionMap.put("/static/**", "anon"); //配置退出 过滤器,其中的具体的退出代码Shiro已经替我们实现了 filterChainDefinitionMap.put("/logout", "logout"); //&lt;!-- 过滤链定义，从上向下顺序执行，一般将/**放在最为下边 --&gt;:这是一个坑呢，一不小心代码就不好使了; //&lt;!-- authc:所有url都必须认证通过才可以访问; anon:所有url都都可以匿名访问--&gt; filterChainDefinitionMap.put("/**", "authc"); // 如果不设置默认会自动寻找Web工程根目录下的"/login.jsp"页面 shiroFilterFactoryBean.setLoginUrl("/login"); // 登录成功后要跳转的链接 shiroFilterFactoryBean.setSuccessUrl("/index"); //未授权界面; shiroFilterFactoryBean.setUnauthorizedUrl("/403"); shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap); return shiroFilterFactoryBean; &#125; @Bean public MyShiroRealm myShiroRealm()&#123; MyShiroRealm myShiroRealm = new MyShiroRealm(); return myShiroRealm; &#125; @Bean public SecurityManager securityManager()&#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); securityManager.setRealm(myShiroRealm()); return securityManager; &#125;&#125; Filter Chain定义说明： 1、一个URL可以配置多个Filter，使用逗号分隔 2、当设置多个过滤器时，全部验证通过，才视为通过 3、部分过滤器可指定参数，如perms，roles Shiro内置的FilterChain: Filter Name Class anon org.apache.shiro.web.filter.authc.AnonymousFilter authc org.apache.shiro.web.filter.authc.FormAuthenticationFilter authcBasic org.apache.shiro.web.filter.authc.BasicHttpAuthenticationFilter perms org.apache.shiro.web.filter.authz.PermissionsAuthorizationFilter port org.apache.shiro.web.filter.authz.PortFilter rest org.apache.shiro.web.filter.authz.HttpMethodPermissionFilter roles org.apache.shiro.web.filter.authz.RolesAuthorizationFilter ssl org.apache.shiro.web.filter.authz.SslFilter user org.apache.shiro.web.filter.authc.UserFilter anon:所有url都都可以匿名访问 authc: 需要认证才能进行访问 user:配置记住我或认证通过可以访问 登录认证实现 在认证、授权内部实现机制中都有提到，最终处理都将交给Real进行处理。因为在Shiro中，最终是通过Realm来获取应用程序中的用户、角色及权限信息的。通常情况下，在Realm中会直接从我们的数据源中获取Shiro需要的验证信息。可以说，Realm是专用于安全框架的DAO.Shiro的认证过程最终会交由Realm执行，这时会调用Realm的getAuthenticationInfo(token)方法。 该方法主要执行以下操作: 1、检查提交的进行认证的令牌信息 2、根据令牌信息从数据源(通常为数据库)中获取用户信息 3、对用户信息进行匹配验证。 4、验证通过将返回一个封装了用户信息的AuthenticationInfo实例。 5、验证失败则抛出AuthenticationException异常信息。 而在我们的应用程序中要做的就是自定义一个Realm类，继承AuthorizingRealm抽象类，重载doGetAuthenticationInfo()，重写获取用户信息的方法。 doGetAuthenticationInfo的重写 12345678910111213141516171819202122@Overrideprotected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123; System.out.println("MyShiroRealm.doGetAuthenticationInfo()"); //获取用户的输入的账号. String username = (String)token.getPrincipal(); System.out.println(token.getCredentials()); //通过username从数据库中查找 User对象，如果找到，没找到. //实际项目中，这里可以根据实际情况做缓存，如果不做，Shiro自己也是有时间间隔机制，2分钟内不会重复执行该方法 UserInfo userInfo = userInfoService.findByUsername(username); System.out.println("-----&gt;&gt;userInfo="+userInfo); if(userInfo == null)&#123; return null; &#125; SimpleAuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo( userInfo, //用户名 userInfo.getPassword(), //密码 ByteSource.Util.bytes(userInfo.getCredentialsSalt()),//salt=username+salt getName() //realm name ); return authenticationInfo;&#125; 链接权限的实现 shiro的权限授权是通过继承AuthorizingRealm抽象类，重载doGetAuthorizationInfo();当访问到页面的时候，链接配置了相应的权限或者shiro标签才会执行此方法否则不会执行，所以如果只是简单的身份认证没有权限的控制的话，那么这个方法可以不进行实现，直接返回null即可。在这个方法中主要是使用类：SimpleAuthorizationInfo进行角色的添加和权限的添加。 12345678910111213@Overrideprotected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; System.out.println("权限配置--&gt;MyShiroRealm.doGetAuthorizationInfo()"); SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo(); UserInfo userInfo = (UserInfo)principals.getPrimaryPrincipal(); for(SysRole role:userInfo.getRoleList())&#123; authorizationInfo.addRole(role.getRole()); for(SysPermission p:role.getPermissions())&#123; authorizationInfo.addStringPermission(p.getPermission()); &#125; &#125; return authorizationInfo;&#125; 当然也可以添加set集合：roles是从数据库查询的当前用户的角色，stringPermissions是从数据库查询的当前用户对应的权限 12authorizationInfo.setRoles(roles);authorizationInfo.setStringPermissions(stringPermissions); 就是说如果在shiro配置文件中添加了filterChainDefinitionMap.put(“/add”, “perms[权限添加]”);就说明访问/add这个链接必须要有“权限添加”这个权限才可以访问，如果在shiro配置文件中添加了filterChainDefinitionMap.put(“/add”, “roles[100002]，perms[权限添加]”);就说明访问/add这个链接必须要有“权限添加”这个权限和具有“100002”这个角色才可以访问。 参考附录:Apache Shiro中文手册 Spring Boot Shiro权限管理【从零开始学Spring Boot】SpringBoot+shiro整合学习之登录认证和权限控制 springboot整合shiro-登录认证和权限管理]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal]]></title>
    <url>%2F2017%2F07%2F22%2Fthread-local%2F</url>
    <content type="text"><![CDATA[ThreadLocal用法Java中线程的同步机制保证了多线程访问共享变量的安全性，通常我们使用synchronized关键字来实现。在多个线程对共享变量进行读写操作时，同步锁保证了同一时间只有一个线程对共享变量进行操作，概括地说，这是一种“以时间换空间”的解决策略。 在JDK1.2中引入了ThreadLocal类来提供了一种“以空间换时间”的同步解决策略。ThreadLocal内部维护了一份类似Map的静态变量ThreadLocalMap，其中key为当前线程，value为共享变量。JDK1.5引入泛型，ThreadLocal也同时支持泛型。其具体实现如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124public class ThreadLocal&lt;T&gt; &#123; /** * ThreadLocals rely on per-thread hash maps attached to each thread * (Thread.threadLocals and inheritableThreadLocals). The ThreadLocal * objects act as keys, searched via threadLocalHashCode. This is a * custom hash code (useful only within ThreadLocalMaps) that eliminates * collisions in the common case where consecutively constructed * ThreadLocals are used by the same threads, while remaining well-behaved * in less common cases. */ private final int threadLocalHashCode = nextHashCode(); /** * The next hash code to be given out. Accessed only by like-named method. */ private static int nextHashCode = 0; /** * The difference between successively generated hash codes - turns * implicit sequential thread-local IDs into near-optimally spread * multiplicative hash values for power-of-two-sized tables. */ private static final int HASH_INCREMENT = 0x61c88647; /** * Compute the next hash code. The static synchronization used here * should not be a performance bottleneck. When ThreadLocals are * generated in different threads at a fast enough rate to regularly * contend on this lock, memory contention is by far a more serious * problem than lock contention. */ private static synchronized int nextHashCode() &#123; int h = nextHashCode; nextHashCode = h + HASH_INCREMENT; return h; &#125; /** * Creates a thread local variable. */ public ThreadLocal() &#123; &#125; /** * Returns the value in the current thread's copy of this thread-local * variable. Creates and initializes the copy if this is the first time * the thread has called this method. * * @return the current thread's value of this thread-local */ public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) return (T)map.get(this); // Maps are constructed lazily. if the map for this thread // doesn't exist, create it, with this ThreadLocal and its // initial value as its only entry. T value = initialValue(); createMap(t, value); return value; &#125; /** * Sets the current thread's copy of this thread-local variable * to the specified value. Many applications will have no need for * this functionality, relying solely on the &#123;@link #initialValue&#125; * method to set the values of thread-locals. * * @param value the value to be stored in the current threads' copy of * this thread-local. */ public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); &#125; /** * Get the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @return the map */ ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals; &#125; /** * Create the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @param firstValue value for the initial entry of the map * @param map the map to store. */ void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue); &#125; ....... /** * ThreadLocalMap is a customized hash map suitable only for * maintaining thread local values. No operations are exported * outside of the ThreadLocal class. The class is package private to * allow declaration of fields in class Thread. To help deal with * very large and long-lived usages, the hash table entries use * WeakReferences for keys. However, since reference queues are not * used, stale entries are guaranteed to be removed only when * the table starts running out of space. */ static class ThreadLocalMap &#123; ........ &#125;&#125; 从中很清晰的可以看出，多个线程拥有自己一份单独的ThreadLocalMap，共享变量对于每个线程都是单独的一份，因此不会造成线程的安全问题。 JDBC的ConnectionManager类就是以这种方式来实现数据库连接Connection对象线程隔离。 1234567891011121314151617181920212223242526272829import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;public class ConnectionManager &#123; private static ThreadLocal&lt;Connection&gt; connectionHolder = new ThreadLocal&lt;Connection&gt;() &#123; @Override protected Connection initialValue() &#123; Connection conn = null; try &#123; conn = DriverManager.getConnection( "jdbc:mysql://localhost:3306/test", "username", "password"); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return conn; &#125; &#125;; public static Connection getConnection() &#123; return connectionHolder.get(); &#125; public static void setConnection(Connection conn) &#123; connectionHolder.set(conn); &#125;&#125; 但是，有些情况ThreadLocal可能并不适用，例如存储大量数据的共享变量，或共享变量只能被创建一次时，就只能通过synchronized来实现了。 推荐阅读]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-cloud入门介绍]]></title>
    <url>%2F2017%2F07%2F22%2Fspring%2Fspring-boot%2Fspring-introduction%2F</url>
    <content type="text"><![CDATA[Spring-cloud入门介绍Spring Cloud官网 Spring Cloud中文网 一、Spring Cloud Netflix二、服务提供与调用 三、熔断器Hystrix四、熔断监控Hystrix Dashboard和Turbine五、配置中心git示例]]></content>
      <categories>
        <category>spring-cloud</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇博客]]></title>
    <url>%2F2017%2F07%2F16%2FMy-first-post%2F</url>
    <content type="text"><![CDATA[为什么我要开始要写博客&nbsp;&nbsp;&nbsp;&nbsp;从15年11月份以来，这一年多在企业工作的日子里，我收获许多。作为一个渴望学技术的程序员，我慢慢摆脱了学校的那种安逸的生活，开始走上了技术宅的道路。 &nbsp;&nbsp;&nbsp;&nbsp;在企业中，前几个月的时间里，我每天都像是海绵一样吸收着养分，学习企业的架构，项目的开发，部署，优化以及维护工作。我每天都痛苦并快乐着，虽然加班，但是我能感觉到自己一点一点的在往上爬。我学会SpringMVC架构，学会使用Maven构建项目，用Ant来实现自动部署项目，用Groovy脚本来编写告警任务。学会了很多软件，诸如MongoDB，Redis等常用开发软件。这个过程中，我很快乐，并且每天都在进步。&nbsp;&nbsp;&nbsp;&nbsp;但是到了17年的3,4月份，我熟悉了团队的各种业务，也明白了项目中所用到的框架和各种技术。我每天做的除了日常的开发和维护，似乎陷入了重复造轮子的困境。虽然在这过程中，我学会了怎样去考虑到新业务或新场景的设计流程和后续的维护过程，但是我始终感觉到了自己的进步慢慢的缓下了，这是我不希望看到的，我渴望进步和成功。 &nbsp;&nbsp;&nbsp;&nbsp;最终，我看到了一句话，“种一棵树，最好的时间是十年前，其次，是现在”，我开始领悟到我必须改变点什么。我开始看基础的JAVA进阶等书籍，开始每天看技术博客或推文，培养自己的兴趣，并且从现在开始，写博客。我以前似乎总在犹豫，我常常害怕自己的技术不够好，但是，从另外一个方面想，这又有什么关系，那就学吧。 &nbsp;&nbsp;&nbsp;&nbsp;为什么写博客，因为它是对你看到，用到知识的升华。在写博客的过程中，不仅会让你review以前的代码，考虑更好的设计方案，也会让你对知识的理解更上一层楼，并记忆深刻。 &nbsp;&nbsp;&nbsp;&nbsp;写代码的时候，往往避免不了遇到各种BUG和技术难点，但是没关系，多请教别人，大多数人愿意和你分享自己的所得。和优秀的人多交流，你们会相互收益。最后，最重要的是，告诉自己不要怕，并且时刻保持一个谦卑的心。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
