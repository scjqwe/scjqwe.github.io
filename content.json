{"meta":{"title":"个人博客","subtitle":"身体和灵魂，总有一个在路上","description":null,"author":"kumu","url":"http://yoursite.com"},"pages":[{"title":"文章标签","date":"2017-08-07T15:26:04.000Z","updated":"2017-08-07T16:05:54.184Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"文章分类","date":"2017-08-07T15:31:10.000Z","updated":"2017-08-07T16:06:10.990Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Kafka知识探索","slug":"kafka","date":"2018-03-11T03:08:00.000Z","updated":"2018-03-11T04:25:59.661Z","comments":true,"path":"2018/03/11/kafka/","link":"","permalink":"http://yoursite.com/2018/03/11/kafka/","excerpt":"环境搭建(Linux)1. Kafka下载 下载地址：http://kafka.apache.org/downloads wget http://apache.fayea.com/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz tar -xvf kafka_2.11-0.10.1.0.tgzcd kafka_2.11-0.10.1.0 2. Zookeeper安装Kafka需要Zookeeper的监控，所以先要安装Zookeeper，如何安装请传送至： hadoop、 zookeeper、 hbase、spark集群环境搭建 ，安装完成以后依次启动各个节点","text":"环境搭建(Linux)1. Kafka下载 下载地址：http://kafka.apache.org/downloads wget http://apache.fayea.com/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz tar -xvf kafka_2.11-0.10.1.0.tgzcd kafka_2.11-0.10.1.0 2. Zookeeper安装Kafka需要Zookeeper的监控，所以先要安装Zookeeper，如何安装请传送至： hadoop、 zookeeper、 hbase、spark集群环境搭建 ，安装完成以后依次启动各个节点#### 3. 配置kafka broker集群+ 首先把Kafka解压后的目录复制到集群的各台服务器+ 然后修改各个服务器的配置文件：进入Kafka的config目录，修改server.properties12345678910# brokerid就是指各台服务器对应的id，所以各台服务器值不同broker.id=0# 端口号，无需改变port=9092# 当前服务器的IP，各台服务器值不同host.name=192.168.0.10# Zookeeper集群的ip和端口号zookeeper.connect=192.168.0.10:2181,192.168.0.11:2181,192.168.0.12:2181# 日志目录log.dirs=/home/www/kafka-logs#### 4. 启动Kafka+ 在每台服务器上进入Kafka目录，分别执行以下命令：1bin/kafka-server-start.sh config/server.properties &amp;#### 5. Kafka常用命令+ ##### 5.1 新建topic1bin/kafka-topics.sh --create --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --replication-factor 2 --partitions 2 --topic test&gt;test有两个复制因子和两个分区+ ##### 5.2 查看某个topic主题1bin/kafka-topics.sh --describe --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --topic test其中第一行是所有分区的信息，下面的每一行对应一个分区Leader：负责某个分区所有读写操作的节点Replicas：复制因子节点Isr：存活节点+ ##### 5.3 查看Kafka所有的主题1bin/kafka-topics.sh --list --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181+ ##### 5.4 终端发送消息1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test+ ##### 5.5 终端接收（消费）消息1bin/kafka-console-consumer.sh --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --bootstrap-server localhost:9092 --topic test --from-beginning### 简介#### 2.1 基本术语##### 消息在Kafka中，每一个消息由键、值和一个时间戳组成##### 主题和日志 Kafka集群存储同一类别的消息流称为主题 主题会有多个订阅者（0个1个或多个），当主题发布消息时，会向订阅者推送记录 针对每一个主题，Kafka集群维护了一个像下面这样的分区日志： 这些分区位于不同的服务器上，每一个分区可以看做是一个结构化的提交日志，每写入一条记录都会记录到其中一个分区并且分配一个唯一地标识其位置的数字称为偏移量offset Kafka集群会将发布的消息保存一段时间，不管是否被消费。例如，如果设置保存天数为2天，那么从消息发布起的两天之内，该消息一直可以被消费，但是超过两天后就会被丢弃以节省空间。其次，Kafka的数据持久化性能很好，所以长时间存储数据不是问题如下图所示，生产者每发布一条消息就会向分区log写入一条记录的offset，而消费者就是通过offset来读取对应的消息的，一般来说每读取一条消息，消费者对应要读取的offset就加1，例如最后一条读到offset=12，那么下条offset就为13.由于消费者通过offset来读取消息，所以可以重复读取已经读过的记录，或者跳过某些记录不读Kafka中采用分区的设计有几个目的。一是可以处理更多的消息，不受单台服务器的限制。Topic拥有多个分区意味着它可以不受限的处理更多的数据。第二，分区可以作为并行处理的单元 分布式Log的分区被分布到集群中的多个服务器上。每个服务器处理它分到的分区。 根据配置每个分区还可以复制到其它服务器作为备份容错每个分区有一个leader，零或多个follower。Leader处理此分区的所有的读写请求，而follower被动的复制数据。如果leader宕机，其它的一个follower会被推举为新的leader。 一台服务器可能同时是一个分区的leader，另一个分区的follower。 这样可以平衡负载，避免所有的请求都只让一台或者某几台服务器处理 生产者生产者往某个Topic上发布消息。生产者还可以选择将消息分配到Topic的哪个节点上。最简单的方式是轮询分配到各个分区以平衡负载，也可以根据某种算法依照权重选择分区 消费者Kafka有一个消费者组的概念，生产者把消息发到的是消费者组，在消费者组里面可以有很多个消费者实例，如下图所示： Kafka集群有两台服务器，四个分区，此外有两个消费者组A和B，消费者组A具有2个消费者实例C1-2，消费者B具有4个消费者实例C3-6那么Kafka发送消息的过程是怎样的呢？例如此时我们创建了一个主题test，有两个分区，分别是Server1的P0和Server2的P1，假设此时我们通过test发布了一条消息，那么这条消息是发到P0还是P1呢，或者是都发呢？答案是只会发到P0或P1其中之一，也就是消息只会发给其中的一个分区分区接收到消息后会记录在分区日志中，记录的方式我们讲过了，就是通过offset，正因为有这个偏移量的存在，所以一个分区内的消息是有先后顺序的，即offset大的消息比offset小的消息后到。但是注意，由于消息随机发往主题的任意一个分区，因此虽然同一个分区的消息有先后顺序，但是不同分区之间的消息就没有先后顺序了，那么如果我们要求消费者顺序消费主题发的消息那该怎么办呢，此时只要在创建主题的时候只提供一个分区即可讲完了主题发消息，接下来就该消费者消费消息了，假设上面test的消息发给了分区P0，此时从图中可以看到，有两个消费者组，那么P0将会把消息发到哪个消费者组呢？从图中可以看到，P0把消息既发给了消费者组A也发给了B，但是A中消息仅被C1消费，B中消息仅被C3消费。这就是我们要讲的，主题发出的消息会发往所有的消费者组，而每一个消费者组下面可以有很多消费者实例，这条消息只会被他们中的一个消费掉 2.2 核心APIKafka具有4个核心API： Producer API：用于向Kafka主题发送消息。 Consumer API：用于从订阅主题接收消息并且处理这些消息。 Streams API：作为一个流处理器，用于从一个或者多个主题中消费消息流然后为其他主题生产消息流，高效地将输入流转换为输出流。 Connector API：用于构建和运行将Kafka主题和已有应用或者数据系统连接起来的可复用的生产者或消费者。例如一个主题到一个关系型数据库的连接能够捕获表的任意变化。 2.3 应用场景Kafka用作消息系统Kafka流的概念与传统企业消息系统有什么异同？传统消息系统有两个模型：队列和发布-订阅系统。在队列模式中，每条服务器的消息会被消费者池中的一个所读取；而发布-订阅系统中消息会广播给所有的消费者。这两种模式各有优劣。队列模式的优势是可以将消息数据让多个消费者处理以实现程序的可扩展，然而这就导致其没有多个订阅者，只能用于一个进程。发布-订阅模式的好处在于数据可以被多个进程消费使用，但是却无法使单一程序扩展性能Kafka中消费者组的概念同时涵盖了这两方面。对应于队列的概念，Kafka中每个消费者组中有多个消费者实例可以接收消息；对应于发布-订阅模式，Kafka中可以指定多个消费者组来订阅消息相对传统消息系统，Kafka可以提供更强的顺序保证 Kafka用作存储系统任何发布消息与消费消息解耦的消息队列其实都可以看做是用来存放发布的消息的存储系统，而Kafka是一个非常高效的存储系统写入Kafka的数据会被存入磁盘并且复制到集群中以容错。Kafka允许生产者等待数据完全复制并且确保持久化到磁盘的确认应答Kafka使用的磁盘结构扩容性能很好——不管服务器上有50KB还是50TB，Kafka的表现都是一样的由于能够精致的存储并且供客户端程序进行读操作，你可以把Kafka看做是一个用于高性能、低延迟的存储提交日志、复制及传播的分布式文件系统 Kafka的流处理仅仅读、写、存储流数据是不够的，Kafka的目的是实现实时流处理。在Kafka中一个流处理器的处理流程是首先持续性的从输入主题中获取数据流，然后对其进行一些处理，再持续性地向输出主题中生产数据流。例如一个销售商应用，接收销售和发货量的输入流，输出新订单和调整后价格的输出流可以直接使用producer和consumer API进行简单的处理。对于复杂的转换，Kafka提供了更强大的Streams API。可构建聚合计算或连接流到一起的复杂应用程序流处理有助于解决这类应用面临的硬性问题：处理无序数据、代码更改的再处理、执行状态计算等Streams API所依托的都是Kafka的核心内容：使用producer和consumer API作为输入，使用Kafka作为状态存储，在流处理实例上使用相同的组机制来实现容错 使用消费者自动提交使用如下api自动提交： 1properties.put(\"enable.auto.commit\", \"false\"); 消费者手动提交每个消费者和对应的patition建立对应的流来读取kafka上面的数据，如果comsumer得到数据，那么kafka就会自动去维护该comsumer的offset，例如在获取到kafka的消息后正准备入库（未入库），但是消费者挂了，那么如果让kafka自动去维护offset，它就会认为这条数据已经被消费了，那么会造成数据丢失。但是kafka可以让你自己去手动提交，如果在上面的场景中，那么需要我们手动commit，如果comsumer挂了 那么程序就不会执行commit这样的话 其他同group的消费者又可以消费这条数据，保证数据不丢，先要做如下设置： 12//设置不自动提交，自己手动更新offsetproperties.put(\"enable.auto.commit\", \"false\"); 使用如下api提交： 1consumer.commitSync();","categories":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"ELK环境搭建","slug":"ELK","date":"2017-08-04T15:05:00.000Z","updated":"2018-03-11T04:27:17.095Z","comments":true,"path":"2017/08/04/ELK/","link":"","permalink":"http://yoursite.com/2017/08/04/ELK/","excerpt":"一、配置系统： Windows 8.1 elasticsearch：5.5.1 logstash：2.0.0 kibana：5.5.1 注：由于实验性搭建，选择windows系统，但选择Linux系统效果更佳 二、部署方案1.ELK+Redis2.ELK+Kafka 注：本次搭建选用第一种方案","text":"一、配置系统： Windows 8.1 elasticsearch：5.5.1 logstash：2.0.0 kibana：5.5.1 注：由于实验性搭建，选择windows系统，但选择Linux系统效果更佳 二、部署方案1.ELK+Redis2.ELK+Kafka 注：本次搭建选用第一种方案 三、安装 前提:下载nssm 1. Elasticsearch下载： download2. logstash下载： download3. kibana下载： download注册为windows服务(a) 将下载的nssm.exe分别拷贝到Elasticsearch、logstash和kibana解压后的bin目录下，然后CMD进入bin执行nssm install 服务名,例如Elasticsearch 的执行nssm install elasticsearch-service..(b) 分析选择path为各压缩包的bin目录下的elasticsearch.bat、logstash.bat和kibana.bat(c) Details选项卡设置显示名为Windows名(d) 最后选择Install service四、部署1. 创建Maven项目elk-log(可另外取名)，pom文件为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.suncj&lt;/groupId&gt; &lt;artifactId&gt;elk-log&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;elk-log&lt;/name&gt; &lt;description&gt;elk日志生成项目&lt;/description&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;4.2.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty.aggregate&lt;/groupId&gt; &lt;artifactId&gt;jetty-all&lt;/artifactId&gt; &lt;version&gt;8.1.19.v20160209&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;4.9&lt;/version&gt; &lt;/dependency&gt; &lt;!--实现slf4j接口并整合 --&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2. 配置logback,logback.xml文件为：1234567891011121314151617181920212223242526&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration debug=\"false\"&gt; &lt;appender name=\"console\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt; &lt;!-- 格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符 --&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %c&#123;1&#125;.%M:%L - %m%n &lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=\"stash\" class=\"net.logstash.logback.appender.LogstashTcpSocketAppender\"&gt; &lt;destination&gt;127.0.0.1:9250&lt;/destination&gt; &lt;encoder charset=\"UTF-8\" class=\"net.logstash.logback.encoder.LogstashEncoder\" /&gt; &lt;/appender&gt; &lt;logger name=\"com.suncj\" level=\"INFO\" /&gt; &lt;root level=\"INFO\"&gt; &lt;appender-ref ref=\"console\" /&gt; &lt;appender-ref ref=\"stash\" /&gt; &lt;/root&gt;&lt;/configuration&gt; 3.设置项目定时任务(打日志)定时任务类LogProducer:12345678910111213141516171819202122232425package com.suncj.elk;import java.util.Random;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * 日志生成器&lt;br&gt; * 版权：Copyright (c) 2015-2016&lt;br&gt; * 创建日期：2017年8月5日&lt;br&gt; */public class LogProducer &#123; private static final Logger log = LoggerFactory.getLogger(LogProducer.class); private Random rand = new Random(); private static int logId = 0; public void produce() &#123; log.info(\"log_id: &#123;&#125; , content:&#123;&#125;\", logId, String.format(\"I am %s\", logId + rand.nextInt(100000))); logId++; &#125;&#125; 项目启动类:123456789101112131415161718192021222324package com.suncj.elk;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Application &#123; private static Logger logger = LoggerFactory.getLogger(Application.class); public static ApplicationContext appContext; public static void main(String[] args) &#123; try &#123; logger.info(\"准备加载程序\"); appContext = new ClassPathXmlApplicationContext(\"app-*.xml\"); logger.info(\"加载完成\"); &#125; catch (Exception e) &#123; logger.error(\"主程序出错:\", e); &#125; &#125;&#125; 其他配置文件：app-task.xml12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:util=\"http://www.springframework.org/schema/util\" xmlns:task=\"http://www.springframework.org/schema/task\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;bean id=\"logProducer\" class=\"com.suncj.elk.LogProducer\"&gt;&lt;/bean&gt; &lt;task:scheduled-tasks&gt; &lt;task:scheduled ref=\"logProducer\" method=\"produce\" cron=\"0/5 * * * * *\" /&gt; &lt;/task:scheduled-tasks&gt;&lt;/beans&gt; 2. logstash配置(a) run_es.bat，run_redis.bat1logstash.bat agent -f logstash_es.conf (b) logstash_redis.conf1234567891011121314151617input &#123; tcp &#123; host =&gt; \"127.0.0.1\" port =&gt; 9250 mode =&gt; \"server\" codec =&gt; json_lines &#125;&#125;output &#123; redis &#123; host =&gt; \"127.0.0.1\" port =&gt; 6379 db =&gt; 1 data_type =&gt; \"list\" key =&gt; \"log:es\" &#125;&#125; (c) logstash_es.conf12345678910111213141516171819input &#123; redis &#123; data_type =&gt; \"list\" key =&gt; \"log:es\" host =&gt; \"127.0.0.1\" db =&gt; 1 port =&gt; 6379 &#125;&#125;output &#123; stdout&#123; codec =&gt; rubydebug &#125; elasticsearch &#123; hosts =&gt; [\"127.0.0.1:9200\"] index =&gt; \"log-es-%&#123;+YYYY.MM.dd&#125;\" flush_size =&gt; 1000 &#125;&#125; 注: logstash注册为windows服务时需要创建两个bat文件，一个用于项目日志存储到redis；另外一个用户读取redis,输出到elasticsearch，因此需要注册两个服务名不同的windows服务 参考资料https://kibana.logstash.es/content/kibana/index.html http://blog.csdn.net/tulizi/article/details/52972824 http://udn.yyuap.com/doc/logstash-best-practice-cn/input/redis.html https://www.elastic.co/guide/en/logstash/current/codec-plugins.html","categories":[{"name":"ELK","slug":"ELK","permalink":"http://yoursite.com/categories/ELK/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://yoursite.com/tags/ELK/"}]},{"title":"Spring-Shiro介绍及其使用","slug":"spring-shiro","date":"2017-08-01T03:17:11.000Z","updated":"2017-08-07T16:46:29.570Z","comments":true,"path":"2017/08/01/spring-shiro/","link":"","permalink":"http://yoursite.com/2017/08/01/spring-shiro/","excerpt":"What is Apache Shiro?Apache Shiro是一个功能强大、灵活的，开源的安全框架。它可以干净利落地处理身份验证、授权、企业会话管理和加密。 Apache Shiro的首要目标是易于使用和理解。安全通常很复杂，甚至让人感到很痛苦，但是Shiro却不是这样子的。一个好的安全框架应该屏蔽复杂性，向外暴露简单、直观的API，来简化开发人员实现应用程序安全所花费的时间和精力。","text":"What is Apache Shiro?Apache Shiro是一个功能强大、灵活的，开源的安全框架。它可以干净利落地处理身份验证、授权、企业会话管理和加密。 Apache Shiro的首要目标是易于使用和理解。安全通常很复杂，甚至让人感到很痛苦，但是Shiro却不是这样子的。一个好的安全框架应该屏蔽复杂性，向外暴露简单、直观的API，来简化开发人员实现应用程序安全所花费的时间和精力。Shiro能做什么呢？ 验证用户身份 用户访问权限控制，比如：1、判断用户是否分配了一定的安全角色。2、判断用户是否被授予完成某个操作的权限 在非 web 或 EJB 容器的环境下可以任意使用Session API 可以响应认证、访问控制，或者 Session 生命周期中发生的事件 可将一个或以上用户安全数据源数据组合成一个复合的用户 “view”(视图) 支持单点登录(SSO)功能 支持提供“Remember Me”服务，获取用户关联信息而无需登录… 等等——都集成到一个有凝聚力的易于使用的API。 Shiro 致力在所有应用环境下实现上述功能，小到命令行应用程序，大到企业应用中，而且不需要借助第三方框架、容器、应用服务器等。当然 Shiro 的目的是尽量的融入到这样的应用环境中去，但也可以在它们之外的任何环境下开箱即用。 Apache Shiro Features 特性Apache Shiro是一个全面的、蕴含丰富功能的安全框架。下图为描述Shiro功能的框架图： Authentication（认证）, Authorization（授权）, Session Management（会话管理）, Cryptography（加密）被 Shiro 框架的开发团队称之为应用安全的四大基石。那么就让我们来看看它们吧： Authentication（认证）：用户身份识别，通常被称为用户“登录” Authorization（授权）：访问控制。比如某个用户是否具有某个操作的使用权限。 Session Management（会话管理）：特定于用户的会话管理,甚至在非web 或 EJB 应用程序。 Cryptography（加密）：在对数据源使用加密算法加密的同时，保证易于使用。 还有其他的功能来支持和加强这些不同应用环境下安全领域的关注点。特别是对以下的功能支持： Web支持：Shiro 提供的 web 支持 api ，可以很轻松的保护 web 应用程序的安全。 缓存：缓存是 Apache Shiro 保证安全操作快速、高效的重要手段。 并发：Apache Shiro 支持多线程应用程序的并发特性。 测试：支持单元测试和集成测试，确保代码和预想的一样安全。 “Run As”：这个功能允许用户假设另一个用户的身份(在许可的前提下)。 “Remember Me”：跨 session 记录用户的身份，只有在强制需要时才需要登录。 注意： Shiro不会去维护用户、维护权限，这些需要我们自己去设计/提供，然后通过相应的接口注入给Shiro High-Level Overview 高级概述在概念层，Shiro 架构包含三个主要的理念：Subject,SecurityManager和 Realm。下面的图展示了这些组件如何相互作用，我们将在下面依次对其进行描述。 Subject：当前用户，Subject 可以是一个人，但也可以是第三方服务、守护进程帐户、时钟守护任务或者其它–当前和软件交互的任何事件。 SecurityManager：管理所有Subject，SecurityManager 是 Shiro 架构的核心，配合内部安全组件共同组成安全伞。 Realms：用于进行权限信息的验证，我们自己实现。Realm 本质上是一个特定的安全 DAO：它封装与数据源连接的细节，得到Shiro 所需的相关的数据。在配置 Shiro 的时候，你必须指定至少一个Realm 来实现认证（authentication）和/或授权（authorization）。 我们需要实现Realms的Authentication 和 Authorization。其中 Authentication 是用来验证用户身份，Authorization 是授权访问控制，用于对用户进行的操作授权，证明该用户是否允许进行当前操作，如访问某个链接，某个资源文件等。 快速上手pom.xml1234567891011121314151617181920212223242526272829&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sourceforge.nekohtml&lt;/groupId&gt; &lt;artifactId&gt;nekohtml&lt;/artifactId&gt; &lt;version&gt;1.9.22&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Shiro 配置1234567891011121314151617181920212223242526272829303132333435363738394041@Configurationpublic class ShiroConfig &#123; @Bean public ShiroFilterFactoryBean shirFilter(SecurityManager securityManager) &#123; System.out.println(\"ShiroConfiguration.shirFilter()\"); ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); shiroFilterFactoryBean.setSecurityManager(securityManager); //拦截器. Map&lt;String,String&gt; filterChainDefinitionMap = new LinkedHashMap&lt;String,String&gt;(); // 配置不会被拦截的链接 顺序判断 filterChainDefinitionMap.put(\"/static/**\", \"anon\"); //配置退出 过滤器,其中的具体的退出代码Shiro已经替我们实现了 filterChainDefinitionMap.put(\"/logout\", \"logout\"); //&lt;!-- 过滤链定义，从上向下顺序执行，一般将/**放在最为下边 --&gt;:这是一个坑呢，一不小心代码就不好使了; //&lt;!-- authc:所有url都必须认证通过才可以访问; anon:所有url都都可以匿名访问--&gt; filterChainDefinitionMap.put(\"/**\", \"authc\"); // 如果不设置默认会自动寻找Web工程根目录下的\"/login.jsp\"页面 shiroFilterFactoryBean.setLoginUrl(\"/login\"); // 登录成功后要跳转的链接 shiroFilterFactoryBean.setSuccessUrl(\"/index\"); //未授权界面; shiroFilterFactoryBean.setUnauthorizedUrl(\"/403\"); shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap); return shiroFilterFactoryBean; &#125; @Bean public MyShiroRealm myShiroRealm()&#123; MyShiroRealm myShiroRealm = new MyShiroRealm(); return myShiroRealm; &#125; @Bean public SecurityManager securityManager()&#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); securityManager.setRealm(myShiroRealm()); return securityManager; &#125;&#125; Filter Chain定义说明： 1、一个URL可以配置多个Filter，使用逗号分隔 2、当设置多个过滤器时，全部验证通过，才视为通过 3、部分过滤器可指定参数，如perms，roles Shiro内置的FilterChain: Filter Name Class anon org.apache.shiro.web.filter.authc.AnonymousFilter authc org.apache.shiro.web.filter.authc.FormAuthenticationFilter authcBasic org.apache.shiro.web.filter.authc.BasicHttpAuthenticationFilter perms org.apache.shiro.web.filter.authz.PermissionsAuthorizationFilter port org.apache.shiro.web.filter.authz.PortFilter rest org.apache.shiro.web.filter.authz.HttpMethodPermissionFilter roles org.apache.shiro.web.filter.authz.RolesAuthorizationFilter ssl org.apache.shiro.web.filter.authz.SslFilter user org.apache.shiro.web.filter.authc.UserFilter anon:所有url都都可以匿名访问 authc: 需要认证才能进行访问 user:配置记住我或认证通过可以访问 登录认证实现 在认证、授权内部实现机制中都有提到，最终处理都将交给Real进行处理。因为在Shiro中，最终是通过Realm来获取应用程序中的用户、角色及权限信息的。通常情况下，在Realm中会直接从我们的数据源中获取Shiro需要的验证信息。可以说，Realm是专用于安全框架的DAO.Shiro的认证过程最终会交由Realm执行，这时会调用Realm的getAuthenticationInfo(token)方法。 该方法主要执行以下操作: 1、检查提交的进行认证的令牌信息 2、根据令牌信息从数据源(通常为数据库)中获取用户信息 3、对用户信息进行匹配验证。 4、验证通过将返回一个封装了用户信息的AuthenticationInfo实例。 5、验证失败则抛出AuthenticationException异常信息。 而在我们的应用程序中要做的就是自定义一个Realm类，继承AuthorizingRealm抽象类，重载doGetAuthenticationInfo()，重写获取用户信息的方法。 doGetAuthenticationInfo的重写 12345678910111213141516171819202122@Overrideprotected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123; System.out.println(\"MyShiroRealm.doGetAuthenticationInfo()\"); //获取用户的输入的账号. String username = (String)token.getPrincipal(); System.out.println(token.getCredentials()); //通过username从数据库中查找 User对象，如果找到，没找到. //实际项目中，这里可以根据实际情况做缓存，如果不做，Shiro自己也是有时间间隔机制，2分钟内不会重复执行该方法 UserInfo userInfo = userInfoService.findByUsername(username); System.out.println(\"-----&gt;&gt;userInfo=\"+userInfo); if(userInfo == null)&#123; return null; &#125; SimpleAuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo( userInfo, //用户名 userInfo.getPassword(), //密码 ByteSource.Util.bytes(userInfo.getCredentialsSalt()),//salt=username+salt getName() //realm name ); return authenticationInfo;&#125; 链接权限的实现 shiro的权限授权是通过继承AuthorizingRealm抽象类，重载doGetAuthorizationInfo();当访问到页面的时候，链接配置了相应的权限或者shiro标签才会执行此方法否则不会执行，所以如果只是简单的身份认证没有权限的控制的话，那么这个方法可以不进行实现，直接返回null即可。在这个方法中主要是使用类：SimpleAuthorizationInfo进行角色的添加和权限的添加。 12345678910111213@Overrideprotected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; System.out.println(\"权限配置--&gt;MyShiroRealm.doGetAuthorizationInfo()\"); SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo(); UserInfo userInfo = (UserInfo)principals.getPrimaryPrincipal(); for(SysRole role:userInfo.getRoleList())&#123; authorizationInfo.addRole(role.getRole()); for(SysPermission p:role.getPermissions())&#123; authorizationInfo.addStringPermission(p.getPermission()); &#125; &#125; return authorizationInfo;&#125; 当然也可以添加set集合：roles是从数据库查询的当前用户的角色，stringPermissions是从数据库查询的当前用户对应的权限 12authorizationInfo.setRoles(roles);authorizationInfo.setStringPermissions(stringPermissions); 就是说如果在shiro配置文件中添加了filterChainDefinitionMap.put(“/add”, “perms[权限添加]”);就说明访问/add这个链接必须要有“权限添加”这个权限才可以访问，如果在shiro配置文件中添加了filterChainDefinitionMap.put(“/add”, “roles[100002]，perms[权限添加]”);就说明访问/add这个链接必须要有“权限添加”这个权限和具有“100002”这个角色才可以访问。 参考附录:Apache Shiro中文手册 Spring Boot Shiro权限管理【从零开始学Spring Boot】SpringBoot+shiro整合学习之登录认证和权限控制 springboot整合shiro-登录认证和权限管理","categories":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"},{"name":"shiro","slug":"shiro","permalink":"http://yoursite.com/tags/shiro/"}]},{"title":"ThreadLocal","slug":"thread-local","date":"2017-07-22T10:54:25.000Z","updated":"2017-08-07T15:53:39.847Z","comments":true,"path":"2017/07/22/thread-local/","link":"","permalink":"http://yoursite.com/2017/07/22/thread-local/","excerpt":"ThreadLocal用法Java中线程的同步机制保证了多线程访问共享变量的安全性，通常我们使用synchronized关键字来实现。在多个线程对共享变量进行读写操作时，同步锁保证了同一时间只有一个线程对共享变量进行操作，概括地说，这是一种“以时间换空间”的解决策略。 在JDK1.2中引入了ThreadLocal类来提供了一种“以空间换时间”的同步解决策略。ThreadLocal内部维护了一份类似Map的静态变量ThreadLocalMap，其中key为当前线程，value为共享变量。JDK1.5引入泛型，ThreadLocal也同时支持泛型。","text":"ThreadLocal用法Java中线程的同步机制保证了多线程访问共享变量的安全性，通常我们使用synchronized关键字来实现。在多个线程对共享变量进行读写操作时，同步锁保证了同一时间只有一个线程对共享变量进行操作，概括地说，这是一种“以时间换空间”的解决策略。 在JDK1.2中引入了ThreadLocal类来提供了一种“以空间换时间”的同步解决策略。ThreadLocal内部维护了一份类似Map的静态变量ThreadLocalMap，其中key为当前线程，value为共享变量。JDK1.5引入泛型，ThreadLocal也同时支持泛型。其具体实现如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124public class ThreadLocal&lt;T&gt; &#123; /** * ThreadLocals rely on per-thread hash maps attached to each thread * (Thread.threadLocals and inheritableThreadLocals). The ThreadLocal * objects act as keys, searched via threadLocalHashCode. This is a * custom hash code (useful only within ThreadLocalMaps) that eliminates * collisions in the common case where consecutively constructed * ThreadLocals are used by the same threads, while remaining well-behaved * in less common cases. */ private final int threadLocalHashCode = nextHashCode(); /** * The next hash code to be given out. Accessed only by like-named method. */ private static int nextHashCode = 0; /** * The difference between successively generated hash codes - turns * implicit sequential thread-local IDs into near-optimally spread * multiplicative hash values for power-of-two-sized tables. */ private static final int HASH_INCREMENT = 0x61c88647; /** * Compute the next hash code. The static synchronization used here * should not be a performance bottleneck. When ThreadLocals are * generated in different threads at a fast enough rate to regularly * contend on this lock, memory contention is by far a more serious * problem than lock contention. */ private static synchronized int nextHashCode() &#123; int h = nextHashCode; nextHashCode = h + HASH_INCREMENT; return h; &#125; /** * Creates a thread local variable. */ public ThreadLocal() &#123; &#125; /** * Returns the value in the current thread's copy of this thread-local * variable. Creates and initializes the copy if this is the first time * the thread has called this method. * * @return the current thread's value of this thread-local */ public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) return (T)map.get(this); // Maps are constructed lazily. if the map for this thread // doesn't exist, create it, with this ThreadLocal and its // initial value as its only entry. T value = initialValue(); createMap(t, value); return value; &#125; /** * Sets the current thread's copy of this thread-local variable * to the specified value. Many applications will have no need for * this functionality, relying solely on the &#123;@link #initialValue&#125; * method to set the values of thread-locals. * * @param value the value to be stored in the current threads' copy of * this thread-local. */ public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); &#125; /** * Get the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @return the map */ ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals; &#125; /** * Create the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @param firstValue value for the initial entry of the map * @param map the map to store. */ void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue); &#125; ....... /** * ThreadLocalMap is a customized hash map suitable only for * maintaining thread local values. No operations are exported * outside of the ThreadLocal class. The class is package private to * allow declaration of fields in class Thread. To help deal with * very large and long-lived usages, the hash table entries use * WeakReferences for keys. However, since reference queues are not * used, stale entries are guaranteed to be removed only when * the table starts running out of space. */ static class ThreadLocalMap &#123; ........ &#125;&#125; 从中很清晰的可以看出，多个线程拥有自己一份单独的ThreadLocalMap，共享变量对于每个线程都是单独的一份，因此不会造成线程的安全问题。 JDBC的ConnectionManager类就是以这种方式来实现数据库连接Connection对象线程隔离。 1234567891011121314151617181920212223242526272829import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;public class ConnectionManager &#123; private static ThreadLocal&lt;Connection&gt; connectionHolder = new ThreadLocal&lt;Connection&gt;() &#123; @Override protected Connection initialValue() &#123; Connection conn = null; try &#123; conn = DriverManager.getConnection( \"jdbc:mysql://localhost:3306/test\", \"username\", \"password\"); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return conn; &#125; &#125;; public static Connection getConnection() &#123; return connectionHolder.get(); &#125; public static void setConnection(Connection conn) &#123; connectionHolder.set(conn); &#125;&#125; 但是，有些情况ThreadLocal可能并不适用，例如存储大量数据的共享变量，或共享变量只能被创建一次时，就只能通过synchronized来实现了。 推荐阅读","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://yoursite.com/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://yoursite.com/tags/JAVA/"}]},{"title":"Spring-cloud入门介绍","slug":"spring-introduction","date":"2017-07-22T10:52:56.000Z","updated":"2017-08-07T15:53:31.568Z","comments":true,"path":"2017/07/22/spring-introduction/","link":"","permalink":"http://yoursite.com/2017/07/22/spring-introduction/","excerpt":"Spring-cloud入门介绍Spring Cloud官网 Spring Cloud中文网 一、Spring Cloud Netflix二、服务提供与调用","text":"Spring-cloud入门介绍Spring Cloud官网 Spring Cloud中文网 一、Spring Cloud Netflix二、服务提供与调用 三、熔断器Hystrix四、熔断监控Hystrix Dashboard和Turbine五、配置中心git示例","categories":[{"name":"spring-cloud","slug":"spring-cloud","permalink":"http://yoursite.com/categories/spring-cloud/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"},{"name":"spring-cloud","slug":"spring-cloud","permalink":"http://yoursite.com/tags/spring-cloud/"}]},{"title":"第一篇博客","slug":"My-first-post","date":"2017-07-16T03:17:11.000Z","updated":"2017-08-07T15:53:35.496Z","comments":true,"path":"2017/07/16/My-first-post/","link":"","permalink":"http://yoursite.com/2017/07/16/My-first-post/","excerpt":"为什么我要开始要写博客&nbsp;&nbsp;&nbsp;&nbsp;从15年11月份以来，这一年多在企业工作的日子里，我收获许多。作为一个渴望学技术的程序员，我慢慢摆脱了学校的那种安逸的生活，开始走上了技术宅的道路。 &nbsp;&nbsp;&nbsp;&nbsp;在企业中，前几个月的时间里，我每天都像是海绵一样吸收着养分，学习企业的架构，项目的开发，部署，优化以及维护工作。我每天都痛苦并快乐着，虽然加班，但是我能感觉到自己一点一点的在往上爬。我学会SpringMVC架构，学会使用Maven构建项目，用Ant来实现自动部署项目，用Groovy脚本来编写告警任务。学会了很多软件，诸如MongoDB，Redis等常用开发软件。这个过程中，我很快乐，并且每天都在进步。","text":"为什么我要开始要写博客&nbsp;&nbsp;&nbsp;&nbsp;从15年11月份以来，这一年多在企业工作的日子里，我收获许多。作为一个渴望学技术的程序员，我慢慢摆脱了学校的那种安逸的生活，开始走上了技术宅的道路。 &nbsp;&nbsp;&nbsp;&nbsp;在企业中，前几个月的时间里，我每天都像是海绵一样吸收着养分，学习企业的架构，项目的开发，部署，优化以及维护工作。我每天都痛苦并快乐着，虽然加班，但是我能感觉到自己一点一点的在往上爬。我学会SpringMVC架构，学会使用Maven构建项目，用Ant来实现自动部署项目，用Groovy脚本来编写告警任务。学会了很多软件，诸如MongoDB，Redis等常用开发软件。这个过程中，我很快乐，并且每天都在进步。&nbsp;&nbsp;&nbsp;&nbsp;但是到了17年的3,4月份，我熟悉了团队的各种业务，也明白了项目中所用到的框架和各种技术。我每天做的除了日常的开发和维护，似乎陷入了重复造轮子的困境。虽然在这过程中，我学会了怎样去考虑到新业务或新场景的设计流程和后续的维护过程，但是我始终感觉到了自己的进步慢慢的缓下了，这是我不希望看到的，我渴望进步和成功。 &nbsp;&nbsp;&nbsp;&nbsp;最终，我看到了一句话，“种一棵树，最好的时间是十年前，其次，是现在”，我开始领悟到我必须改变点什么。我开始看基础的JAVA进阶等书籍，开始每天看技术博客或推文，培养自己的兴趣，并且从现在开始，写博客。我以前似乎总在犹豫，我常常害怕自己的技术不够好，但是，从另外一个方面想，这又有什么关系，那就学吧。 &nbsp;&nbsp;&nbsp;&nbsp;为什么写博客，因为它是对你看到，用到知识的升华。在写博客的过程中，不仅会让你review以前的代码，考虑更好的设计方案，也会让你对知识的理解更上一层楼，并记忆深刻。 &nbsp;&nbsp;&nbsp;&nbsp;写代码的时候，往往避免不了遇到各种BUG和技术难点，但是没关系，多请教别人，大多数人愿意和你分享自己的所得。和优秀的人多交流，你们会相互收益。最后，最重要的是，告诉自己不要怕，并且时刻保持一个谦卑的心。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]}]}