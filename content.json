{"meta":{"title":"个人博客","subtitle":"身体和灵魂，总有一个在路上","description":null,"author":"kumu","url":"http://yoursite.com"},"pages":[{"title":"文章分类","date":"2017-08-07T15:31:10.000Z","updated":"2017-08-07T16:06:10.990Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"文章标签","date":"2017-08-07T15:26:04.000Z","updated":"2017-08-07T16:05:54.184Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"JVM进程常用参数","slug":"java/java-param","date":"2018-07-24T14:29:00.000Z","updated":"2018-07-24T14:44:42.856Z","comments":true,"path":"2018/07/24/java/java-param/","link":"","permalink":"http://yoursite.com/2018/07/24/java/java-param/","excerpt":"查看JDK参数查看并行收集线程/usr/jdk1.8.0_101/bin/java -server -Xmx1024m -Xms1024m -XX:+UseConcMarkSweepGC -XX:+PrintFlagsFinal -version| grep ParallelGCThreads","text":"查看JDK参数查看并行收集线程/usr/jdk1.8.0_101/bin/java -server -Xmx1024m -Xms1024m -XX:+UseConcMarkSweepGC -XX:+PrintFlagsFinal -version| grep ParallelGCThreads 内存控制参数 参数名 说明 -Xms128m 初始的内存 -Xmx256m 最大堆内存 -Xmn128m 设置新生代内存,和NewRatio有相同的作用,可以不用设置 -Xss256k 线程栈Stack Space,一般情况下可以给256k,实测最小228k,JDK默认给的是1MB的内存 如果多层递归发现StackOverFlow,可以适当调大,调小这个参数可以开启更多的线程 -XX参数-XX后面有部分参数可以跟+或者-,+表示开启,-表示禁用比如-XX:+UseGCOverheadLimit,就表示GC时间过长抛OOM-XX:-UseGCOverheadLimit,就表示GC时间过长不要抛OOM 参数名 说明 -XX:PermSize=64m 初始的永久区大小,&lt;=JDK7可用 -XX:MaxMaxPermSize=128m 最大的永久区大小,&lt;=JDK7可用 -XX:MetaspaceSize=64m 初始的永久区大小,&gt;=JDK8可用 -XX:MaxMetaspaceSize=256m 最大的永久区大小,&gt;=JDK8可用 -XX:NewRatio=1 新生代占1/2,默认情况,新生代占1/3,公式为1/(1+n) -XX:-UseBiasedLocking 取消偏向锁,在synchronized上的 优化,取消对性能有提升 -XX:AutoBoxCacheMax=20000 Integer i = 3;这语句有着 int自动装箱成Integer的过程，JDK默认只缓存 -128 ~ +127的int 和 long -XX:+PerfDisablesharedMem 禁止在/tmp/hperf[用户名] 路径上写入进程统计文件,jps,jstat,远程查看jvm状态也不能用了 -XX:-UseGCOverheadLimit 限制GC的运行时间。如果GC耗时过长，就抛OOM -XX:-OmitStackTraceInFastThrow 2W次异常后,不打印异常栈,建议禁用,不方便查问题 -XX:+UseParNewGC 新生代并行收集 -XX:+UseConcMarkSweepGC 老年代并发收集 -XX:CMSInitiatingOccupancyFraction=75 老年代收集的触发的比例 -XX:+UseCMSInitiatingOccupancyOnly 强制使用上面的触发比例 -XX:MaxTenuringThreshold=2 对象在Survivor区熬过多少次Young GC后晋升到年老代 -XX:+HeapDumpOnOutOfMemoryError 如果出现OutOfMemory,dump内存到文件中,默认情况下dump到当前用户目录 -XX:HeapDumpPath=/home/checkin/dump 指定dump的目录 -XX:+ExplicitGCInvokesConcurrent 显式的System.gc()的调用会使用CMS方式 -XX:+DisableExplicitGC 不允许显式调用System.gc(),此选项最好不要加,有些堆外内存分配依赖显式 gc 调用,关闭之后容易导致 OOM -XX:+PrintGCDetails 打印GC详细情况 -XX:+PrintGCDateStamps 打印GC的时间戳 -verbose:gc 打印gc日志 -Xloggc:gc.log 日志存放在当前路径下,文件名为gc.log,可以使用相对路径和绝对路径 JDK1.7及以下版本参数-Xms128m -Xmx512m -Xss256k -server -XX:PermSize=32m -XX:MaxPermSize=256m -XX:NewRatio=1 -XX:-UseBiasedLocking -XX:AutoBoxCacheMax=20000 -XX:-UseGCOverheadLimit -XX:-OmitStackTraceInFastThrow -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:MaxTenuringThreshold=2 -XX:+HeapDumpOnOutOfMemoryError -XX:+ExplicitGCInvokesConcurrent -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:gc.log JDK1.8及以上版本参数-Xms128m -Xmx512m -Xss256k -server -XX:MetaspaceSize=32m -XX:MaxMetaspaceSize=256m -XX:NewRatio=1 -XX:-UseBiasedLocking -XX:AutoBoxCacheMax=20000 -XX:-UseGCOverheadLimit -XX:-OmitStackTraceInFastThrow -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:MaxTenuringThreshold=2 -XX:+HeapDumpOnOutOfMemoryError -XX:+ExplicitGCInvokesConcurrent -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:gc.log 参考关键业务系统的JVM参数推荐(2016热冬版) http://calvin1978.blogcn.com/articles/jvmoption-2.htmlJVM调优总结 -Xms -Xmx -Xmn -Xss http://unixboy.iteye.com/blog/174173/java高分局之JVM命令参数大全（高级垃圾回收选项）http://blog.csdn.net/maosijunzi/article/details/46562489","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://yoursite.com/tags/jvm/"}]},{"title":"MySQL索引原理","slug":"mysql/index","date":"2018-07-15T15:53:00.000Z","updated":"2018-07-24T14:53:06.500Z","comments":true,"path":"2018/07/15/mysql/index/","link":"","permalink":"http://yoursite.com/2018/07/15/mysql/index/","excerpt":"数据结构二叉排序树（Binary Sort Tree）规则 若左子树不空，则左子树上所有节点的值均小于它的根节点的值 若右子树不空，则右子树上所有节点的值均大于它的根节点的值 它的左、右子树也分别为二叉排序树（递归定义） 说明二叉查找树查找比较方便，因为每次经过一次节点时，最多减少一半的可能。极端情况下，会出现所有节点位于同一侧的情况，直观上看就是一条直线，这种情况的查询效率比较低。因此需要对二叉树左右子树的高度作平衡化处理，这就是平衡二叉树。","text":"数据结构二叉排序树（Binary Sort Tree）规则 若左子树不空，则左子树上所有节点的值均小于它的根节点的值 若右子树不空，则右子树上所有节点的值均大于它的根节点的值 它的左、右子树也分别为二叉排序树（递归定义） 说明二叉查找树查找比较方便，因为每次经过一次节点时，最多减少一半的可能。极端情况下，会出现所有节点位于同一侧的情况，直观上看就是一条直线，这种情况的查询效率比较低。因此需要对二叉树左右子树的高度作平衡化处理，这就是平衡二叉树。 平衡二叉树（Balance Binary Tree）规则 左右子树的高度差的绝对值不超过1 左右子树都是平衡二叉树（递归定义） 说明常见的实现方式为：红黑树，平衡二叉查找树（AVL）,替罪羊树，树堆（Treap），伸展树。在这样的平衡树中进行查找，总共比较节点的次数不超过树的高度，查询效率得到提高，时间复杂度为O(logn)。 平衡多路查找树（B树或B-树）规则 每个节点至多可以拥有m棵子树 根节点，只有至少2个节点 非根非叶的节点至少有Ceil(m/2)个子树（Ceil表示向上取整），例如5阶B树，每个节点至少3个子树 所有叶子节点位于同一层，意思是从根到叶子节点的每一条路径都有同样的长度 说明B树查询与二叉排序树类似，从根节点依次比较每个节点，因为每个节点中关键字和左右子树都是有序的。 B+树规则 有n棵子树的节点含有n个关键字，每个关键字不保存数据，只用来索引，所有数据保存在叶子节点 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接 非叶子节点看成是索引部分，结点中仅含其子树（根节点）中的最大（或最小）关键字 说明B+树查找过程与B树类似，只不过查找时，如果在非叶子节点上的关键字等于给定值，并不终止，而是继续沿着指针直到叶子节点。因此对于B+树，不管查找成功或失败，每次查找都是走了一条从根到叶子节点的路径。通常在B+Tree上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节点（即数据节点）之间是一种链式环结构（双向循环链表）。因此可以对B+Tree进行两种查找运算：一种是对于主键的范围查找和分页查找，另一种是从根节点开始，进行随机查找。 实现MyISAM索引实现MyISAM引擎使用B+Tree作为索引结构，叶结点的data域存放的是数据记录的地址。 主索引(主键) 辅助索引在 MyISAM 中,主索引和辅助索引(Secondary key)在结构上没有任何区别，只是主索引要求 key 是唯一的,而辅助索引的 key 可以重复。 说明同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶结点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 主索引(主键) 辅助索引 说明InnoDB存储引擎中页的大小为16KB，一般表的主键类型为INT（占用4个字节）或BIGINT（占用8个字节），指针类型也一般为4或8个字节，也就是说一个页（B+Tree中的一个节点）中大概存储16KB/(8B+8B)=1K个键值（因为是估值，为方便计算，这里的K取值为10^3）。也就是说一个深度为3的B+Tree索引可以维护10^3 10^3 10^3 = 10亿 条记录。实际情况中每个节点可能不能填充满，因此在数据库中，B+Tree的高度一般都在2~4层。mysql的InnoDB存储引擎在设计时是将根节点常驻内存的，也就是说查找某一键值的行记录时最多只需要1~3次磁盘I/O操作。InnoDB 要求表必须有主键(MyISAM 可以没有)，如果没有显式指定，则 MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键,如果不存在这种列，则MySQL 自动为 InnoDB 表生成一个隐含字段作为主键,类型为长整形。同时,请尽量在 InnoDB 上采用自增字段做表的主键。因为 InnoDB 数据文件本身是一棵B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持 B+Tree 的特性而频繁的分裂调整，十分低效,而使用自增字段作为主键则是一个很好的选择。如果表使用自增主键,那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满,就会自动开辟一个新的页。 聚簇索引InnoDB 使用的是聚簇索引，将主键组织到一棵B+树中， 而行数据就储存在叶子节点上， 若使用”where id = 14”这样的条件查找主键，则按照 B+树的检索算法即可查找到对应的叶节点，之后获得行数据。 若对 Name 列进行条件搜索，则需要两个步骤: 第一步、在辅助索引 B+树中检索 Name，到达其叶子节点获取对应的主键。 第二步、使用主键在主索引B+树种再执行一次 B+树检索操作，最终到达叶子节点即可获取整行数据。 非聚簇索引MyISM 使用的是非聚簇索引, 非聚簇索引的两棵 B+树看上去没什么不同, 节点的结构完全一致只是存储的内容不同而已, 主键索引 B+树的节点存储了主键, 辅助键索引B+树存储了辅助键。 表数据存储在独立的地方, 这两颗 B+树的叶子节点都使用一个地址指向真正的表数据, 对于表数据来说, 这两个键没有任何差别。 由于索引树是独立的, 通过辅助键检索无需访问主键的索引树。","categories":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/categories/mysql/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://yoursite.com/tags/索引/"}]},{"title":"缓存常见问题","slug":"cache/cache-question","date":"2018-04-18T16:00:00.000Z","updated":"2018-04-18T17:04:25.471Z","comments":true,"path":"2018/04/19/cache/cache-question/","link":"","permalink":"http://yoursite.com/2018/04/19/cache/cache-question/","excerpt":"缓存穿透含义缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。","text":"缓存穿透含义缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。 缓存穿透问题可能会使后端存储负载加大，由于很多后端存储不具备高并发性，甚至可能造成后端存储宕掉。通常可以在程序中分别统计总调用数、缓存层命中数、存储层命中数，如果发现大量存储层空命中，可能就是出现了缓存穿透问题。 解决方案缓存空对象存储层不命中后，仍然将空对象保留到缓存层中，之后再访问这个数据将会从缓存中获取，保护了后端数据源。 缓存空对象会有两个问题： 第一，空值做了缓存，意味着缓存层中存了更多的键，需要更多的内存空间 ( 如果是攻击，问题更严重 )，比较有效的方法是针对这类数据设置一个较短的过期时间，让其自动剔除。 第二，缓存层和存储层的数据会有一段时间窗口的不一致，可能会对业务有一定影响。例如过期时间设置为 5 分钟，如果此时存储层添加了这个数据，那此段时间就会出现缓存层和存储层数据的不一致，此时可以利用消息系统或者其他方式清除掉缓存层中的空对象。 实现代码： 布隆过滤对所有可能查询的参数以hash形式存储，在控制层先进行校验，不符合则丢弃。还有最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。 例如： 一个个性化推荐系统有 4 亿个用户 ID，每个小时算法工程师会根据每个用户之前历史行为做出来的个性化放到存储层中，但是最新的用户由于没有历史行为，就会发生缓存穿透的行为，为此可以将所有有个性化推荐数据的用户做成布隆过滤器。如果布隆过滤器认为该用户 ID 不存在，那么就不会访问存储层，在一定程度保护了存储层。 有关布隆过滤器的相关知识，可以参考： https://en.wikipedia.org/wiki/Bloom_filter 可以利用 Redis 的 Bitmaps 实现布隆过滤器，GitHub 上已经开源了类似的方案：https://github.com/erikdubbelboer/Redis-Lua-scaling-bloom-filter 这种方法适用于数据命中不高，数据相对固定实时性低（通常是数据集较大）的应用场景，代码维护较为复杂，但是缓存空间占用少。 对比 缓存雪崩含义缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重雪崩。 解决方案思路预防和解决缓存雪崩问题，可以从以下三个方面进行着手。 1）保证缓存层服务高可用性。 和飞机都有多个引擎一样，如果缓存层设计成高可用的，即使个别节点、个别机器、甚至是机房宕掉，依然可以提供服务，例如前面介绍过的 Redis Sentinel 和 Redis Cluster 都实现了高可用。 2）依赖隔离组件为后端限流并降级。 无论是缓存层还是存储层都会有出错的概率，可以将它们视同为资源。作为并发量较大的系统，假如有一个资源不可用，可能会造成线程全部 hang 在这个资源上，造成整个系统不可用。降级在高并发系统中是非常正常的：比如推荐服务中，如果个性化推荐服务不可用，可以降级补充热点数据，不至于造成前端页面是开天窗。 在实际项目中，我们需要对重要的资源 ( 例如 Redis、 MySQL、 Hbase、外部接口 ) 都进行隔离，让每种资源都单独运行在自己的线程池中，即使个别资源出现了问题，对其他服务没有影响。但是线程池如何管理，比如如何关闭资源池，开启资源池，资源池阀值管理，这些做起来还是相当复杂的，这里推荐一个 Java 依赖隔离工具 Hystrix(https://github.com/Netflix/Hystrix) 方案加锁排队在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。 业界比较常用的做法，是使用mutex。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法 数据预热可以通过缓存reload机制，预先去更新缓存，再即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀 做二级缓存，或者双缓存策略。A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期。 缓存永远不过期这里的“永远不过期”包含两层意思： 1）从缓存上看，确实没有设置过期时间，这就保证了，不会出现热点key过期问题，也就是“物理”不过期。 2）从功能上看，如果不过期，那不就成静态的了吗？所以我们把过期时间存在key对应的value里，如果发现要过期了，通过一个后台的异步线程进行缓存的构建，也就是“逻辑”过期. 参考http://mp.weixin.qq.com/s/TBCEwLVAXdsTszRVpXhVughttps://blog.csdn.net/fei33423/article/details/79027790","categories":[{"name":"cache","slug":"cache","permalink":"http://yoursite.com/categories/cache/"}],"tags":[{"name":"cache","slug":"cache","permalink":"http://yoursite.com/tags/cache/"}]},{"title":"数据库中的锁","slug":"mysql/lock","date":"2018-04-17T15:19:00.000Z","updated":"2018-04-17T16:02:32.942Z","comments":true,"path":"2018/04/17/mysql/lock/","link":"","permalink":"http://yoursite.com/2018/04/17/mysql/lock/","excerpt":"简介在数据库的锁机制中介绍过，数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。 乐观并发控制(乐观锁) 和 悲观并发控制（悲观锁） 是并发控制主要采用的技术手段。","text":"简介在数据库的锁机制中介绍过，数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。 乐观并发控制(乐观锁) 和 悲观并发控制（悲观锁） 是并发控制主要采用的技术手段。 无论是悲观锁还是乐观锁，都是人们定义出来的概念，可以认为是一种思想。其实不仅仅是关系型数据库系统中有乐观锁和悲观锁的概念，像memcache、hibernate、tair等都有类似的概念。 针对于不同的业务场景，应该选用不同的并发控制方式。所以，不要把乐观并发控制和悲观并发控制狭义的理解为DBMS中的概念，更不要把他们和数据中提供的锁机制（行锁、表锁、排他锁、共享锁）混为一谈。其实，在DBMS中，悲观锁正是利用数据库本身提供的锁机制来实现的。 悲观锁 在关系数据库管理系统里，悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）是一种并发控制的方法。它可以阻止一个事务以影响其他用户的方式来修改数据。如果一个事务执行的操作都某行数据应用了锁，那只有当这个事务把锁释放，其他事务才能够执行与该锁冲突的操作。悲观并发控制主要用于数据争用激烈的环境，以及发生并发冲突时使用锁保护数据的成本要低于回滚事务的成本的环境中。 含义悲观锁，正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度(悲观)，因此，在整个数据处理过程中，将数据处于锁定状态。 悲观锁的实现，往往依靠数据库提供的锁机制 （也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据） 实现在数据库中，悲观锁的流程如下： 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。 其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。 应用（MySQL InnoDB中使用悲观锁） 要使用悲观锁，我们必须关闭mysql数据库的自动提交属性，因为MySQL默认使用autocommit模式，也就是说，当你执行一个更新操作后，MySQL会立刻将结果进行提交。set autocommit=0; 12345678910//0.开始事务begin;/begin work;/start transaction; (三者选一就可以)//1.查询出商品信息select status from t_goods where id=1 for update;//2.根据商品信息生成订单insert into t_orders (id,goods_id) values (null,1);//3.修改商品status为2update t_goods set status=2;//4.提交事务commit;/commit work; 上面的查询语句中，我们使用了select…for update的方式，这样就通过开启排他锁的方式实现了悲观锁。此时在t_goods表中，id为1的 那条数据就被我们锁定了，其它的事务必须等本次事务提交之后才能执行。这样我们可以保证当前的数据不会被其它事务修改。 上面我们提到，使用select…for update会把数据给锁住，不过我们需要注意一些锁的级别，MySQL InnoDB默认行级锁。行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁把整张表锁住，这点需要注意。 优点与不足悲观并发控制实际上是“先取锁再访问”的保守策略，为数据处理的安全提供了保证。但是在效率方面，处理加锁的机制会让数据库产生额外的开销，还有增加产生死锁的机会；另外，在只读型事务处理中由于不会产生冲突，也没必要使用锁，这样做只能增加系统负载；还有会降低了并行性，一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以处理那行数 乐观锁 在关系数据库管理系统里，乐观并发控制（又名“乐观锁”，Optimistic Concurrency Control，缩写“OCC”）是一种并发控制的方法。它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。乐观事务控制最早是由孔祥重（H.T.Kung）教授提出。 含义乐观锁（ Optimistic Locking ） 相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。 相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。 数据版本,为数据增加的一个版本标识。当读取数据时，将版本标识的值一同读出，数据每更新一次，同时对版本标识进行更新。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的版本标识进行比对，如果数据库表当前版本号与第一次取出来的版本标识值相等，则予以更新，否则认为是过期数据。 实现实现数据版本有两种方式，第一种是使用版本号，第二种是使用时间戳。 使用版本号时，可以在数据初始化时指定一个版本号，每次对数据的更新操作都对版本号执行+1操作。并判断当前版本号是不是该数据的最新的版本号。 应用（MySQL中使用乐观锁）12345671.查询出商品信息select (status,status,version) from t_goods where id=#&#123;id&#125;2.根据商品信息生成订单3.修改商品status为2update t_goodsset status=2,version=version+1where id=#&#123;id&#125; and version=#&#123;version&#125;; 优点与不足乐观并发控制相信事务之间的数据竞争(data race)的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。但如果直接简单这么做，还是有可能会遇到不可预期的结果，例如两个事务都读取了数据库的某一行，经过修改以后写回数据库，这时就遇到了问题。","categories":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/categories/mysql/"}],"tags":[{"name":"lock","slug":"lock","permalink":"http://yoursite.com/tags/lock/"}]},{"title":"spring-boot整合kafka","slug":"kafka/springboot-kafka","date":"2018-03-27T17:09:00.000Z","updated":"2018-04-01T05:46:34.779Z","comments":true,"path":"2018/03/28/kafka/springboot-kafka/","link":"","permalink":"http://yoursite.com/2018/03/28/kafka/springboot-kafka/","excerpt":"生产者说明KafkaTemplate封装了一个生成器，并提供了方便的方法来发送数据到kafka主题。 提供了异步和同步方法，异步方法返回一个Future。","text":"生产者说明KafkaTemplate封装了一个生成器，并提供了方便的方法来发送数据到kafka主题。 提供了异步和同步方法，异步方法返回一个Future。 其构造方法有： 123456789101112131415ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; sendDefault(V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; sendDefault(K key, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; sendDefault(int partition, K key, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; send(String topic, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; send(String topic, K key, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; send(String topic, int partition, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; send(String topic, int partition, K key, V data);ListenableFuture&lt;SendResult&lt;K, V&gt;&gt; send(Message&lt;?&gt; message); 前3个方法需要向Temple提供默认主题 配置使用Producer配置类 1234567891011121314151617181920212223242526272829303132333435363738394041@Configuration@EnableKafkapublic class ProducerConfig &#123; @Value(\"$&#123;kafka.producer.servers&#125;\") private String servers; @Value(\"$&#123;kafka.producer.retries&#125;\") private int retries; @Value(\"$&#123;kafka.producer.batch.size&#125;\") private int batchSize; @Value(\"$&#123;kafka.producer.linger&#125;\") private int linger; @Value(\"$&#123;kafka.producer.buffer.memory&#125;\") private int bufferMemory; public Map&lt;String, Object&gt; producerConfigs() &#123; Map&lt;String, Object&gt; props = new HashMap&lt;&gt;(); props.put(Config.BOOTSTRAP_SERVERS_CONFIG, servers); props.put(Config.RETRIES_CONFIG, retries); props.put(Config.BATCH_SIZE_CONFIG, batchSize); props.put(Config.LINGER_MS_CONFIG, linger); props.put(Config.BUFFER_MEMORY_CONFIG, bufferMemory); props.put(Config.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); props.put(Config.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); props.put(Config.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class); return props; &#125; public ProducerFactory&lt;String, String&gt; producerFactory() &#123; return new DefaultKafkaProducerFactory&lt;&gt;(producerConfigs()); &#125; @Bean public KafkaTemplate&lt;String, String&gt; kafkaTemplate() &#123; return new KafkaTemplate&lt;String, String&gt;(producerFactory()); &#125;&#125; 示例1234567891011121314151617181920212223@RestController@RequestMapping(\"/kafka/producer\")public class ProducerController &#123; private static Logger logger = LoggerFactory.getLogger(ProducerController.class); @Value(\"$&#123;topic.name&#125;\") private String topicName; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; @RequestMapping(\"/send\") public Object sendKafka(String message) &#123; try &#123; logger.info(\"send kafka message: &#123;&#125;\", message); kafkaTemplate.send(topicName, UUID.randomUUID().toString(), message); return \"success\"; &#125; catch (Exception e) &#123; logger.error(\"发送kafka失败\", e); return \"fail\"; &#125; &#125;&#125; 消费者说明可以通过配置MessageListenerContainer并提供MessageListener或通过使用@KafkaListener注释来接收消息。MessageListenerContainer有两个实现： KafkaMessageListenerContainer：从单个线程上的所有主题/分区接收所有消息ConcurrentMessageListenerContainer：委托给1个或多个KafkaMessageListenerContainer以提供多线程消费。通过container.setConcurrency(3)，来设置多个线程 配置使用Consumer配置类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879@Configuration@EnableKafkapublic class ConsumerConfig &#123; @Value(\"$&#123;kafka.consumer.servers&#125;\") private String servers; @Value(\"$&#123;kafka.consumer.enable.auto.commit&#125;\") private boolean enableAutoCommit; @Value(\"$&#123;kafka.consumer.session.timeout&#125;\") private String sessionTimeout; @Value(\"$&#123;kafka.consumer.auto.commit.interval&#125;\") private String autoCommitInterval; @Value(\"$&#123;kafka.consumer.group.id&#125;\") private String groupId; @Value(\"$&#123;kafka.consumer.topic&#125;\") private String topic; @Value(\"$&#123;kafka.consumer.auto.offset.reset&#125;\") private String autoOffsetReset; @Value(\"$&#123;kafka.consumer.concurrency&#125;\") private int concurrency; /** * KafkaMessageListenerContainer： 从单个线程上的所有主题/分区接收所有消息 @Bean(initMethod = \"doStart\") public KafkaMessageListenerContainer&lt;String, String&gt; kafkaMessageListenerContainer() &#123; KafkaMessageListenerContainer&lt;String, String&gt; container = new KafkaMessageListenerContainer&lt;&gt;(consumerFactory(), containerProperties()); return container; &#125; */ /** * ConcurrentMessageListenerContainer： * 委托给1个或多个KafkaMessageListenerContainer以提供多线程消费。 * 通过container.setConcurrency(3)，来设置多个线程 */ @Bean(initMethod = \"doStart\") public ConcurrentMessageListenerContainer&lt;String, String&gt; concurrentMessageListenerContainer() &#123; ConcurrentMessageListenerContainer&lt;String, String&gt; container = new ConcurrentMessageListenerContainer&lt;&gt;(consumerFactory(), containerProperties()); container.setConcurrency(concurrency); return container; &#125; public ConsumerFactory&lt;String, String&gt; consumerFactory() &#123; return new DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs()); &#125; public ContainerProperties containerProperties() &#123; ContainerProperties containerProperties = new ContainerProperties(topic); containerProperties.setMessageListener(messageListener()); return containerProperties; &#125; public Map&lt;String, Object&gt; consumerConfigs() &#123; Map&lt;String, Object&gt; propsMap = new HashMap&lt;&gt;(); propsMap.put(Config.BOOTSTRAP_SERVERS_CONFIG, servers); propsMap.put(Config.ENABLE_AUTO_COMMIT_CONFIG, enableAutoCommit); propsMap.put(Config.AUTO_COMMIT_INTERVAL_MS_CONFIG, autoCommitInterval); propsMap.put(Config.SESSION_TIMEOUT_MS_CONFIG, sessionTimeout); propsMap.put(Config.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); propsMap.put(Config.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); propsMap.put(Config.GROUP_ID_CONFIG, groupId); propsMap.put(Config.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset); return propsMap; &#125; public MessageListener&lt;String, String&gt; messageListener() &#123; return new CustomMessageListener(); &#125;&#125; 消息接收Java实现直接使用kafka0.10 client去收发消息 123456789101112131415161718192021222324252627@Testpublic void receive()&#123; Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, broker); props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId); props.put(ConsumerConfig.CLIENT_ID_CONFIG, clientId); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"true\"); props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, \"1000\"); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); try&#123; consumer.subscribe(Arrays.asList(topic)); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(10000); records.forEach(record -&gt; &#123; System.out.printf(\"client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n\", clientId, record.topic(), record.partition(), record.offset(), record.key(), record.value()); &#125;); &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125;finally &#123; consumer.close(); &#125;&#125; 使用MessageListener接口继承MessageListener接口 12345678910public class CustomMessageListener implements MessageListener&lt;Integer, String&gt; &#123; private static Logger logger = LoggerFactory.getLogger(CustomMessageListener.class); @Override public void onMessage(ConsumerRecord&lt;Integer, String&gt; data) &#123; logger.info(\"received key: &#123;&#125;, value: &#123;&#125;\", data.key(), data.value()); &#125; //或包含消费者的onMessage方法，以手动提交ofset&#125; 使用@KafkaListener注解12345678910111213@KafkaListener(id = \"foo\", topics = \"myTopic\")public void listen(String data) &#123; ...&#125;@KafkaListener(id = \"bar\", topicPartitions = &#123; @TopicPartition(topic = \"topic1\", partitions = &#123; \"0\", \"1\" &#125;), @TopicPartition(topic = \"topic2\", partitions = \"0\", partitionOffsets = @PartitionOffset(partition = \"1\", initialOffset = \"100\")) &#125;)public void listen(ConsumerRecord&lt;?, ?&gt; record) &#123; ...&#125; 总结 对于生产者来说，封装KafkaProducer到KafkaTemplate相对简单 对于消费者来说，由于spring是采用注解的形式去标注消息处理方法 先在KafkaListenerAnnotationBeanPostProcessor中扫描bean，然后注册到KafkaListenerEndpointRegistrar 而KafkaListenerEndpointRegistrar在afterPropertiesSet的时候去创建MessageListenerContainer messageListener包含了原始endpoint携带的bean以及method转换成的InvocableHandlerMethod ConcurrentMessageListenerContainer这个衔接上，根据配置的spring.kafka.listener.concurrency来生成多个并发的KafkaMessageListenerContainer实例 每个KafkaMessageListenerContainer都自己创建一个ListenerConsumer，然后自己创建一个独立的kafka consumer，每个ListenerConsumer在线程池里头运行，这样来实现并发 每个ListenerConsumer里头都有一个recordsToProcess队列，从原始的kafka consumer poll出来的记录会放到这个队列里头， 然后有一个ListenerInvoker线程循环超时等待从recordsToProcess取出记录，然后调用messageListener的onMessage方法(即KafkaListener注解标准的方法) 项目源码https://github.com/scjqwe/spring-kafka-examples 参考https://docs.spring.io/spring-kafka/docs/1.0.4.RELEASE/reference/html/_reference.htmlhttps://segmentfault.com/a/1190000011471181http://www.2bowl.info/apache-kafka%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%E4%BA%8C-spring%E6%95%B4%E5%90%88kafka/","categories":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"},{"name":"spring-boot","slug":"spring-boot","permalink":"http://yoursite.com/tags/spring-boot/"}]},{"title":"Git知识","slug":"git","date":"2018-03-18T15:21:00.000Z","updated":"2018-03-19T16:27:44.034Z","comments":true,"path":"2018/03/18/git/","link":"","permalink":"http://yoursite.com/2018/03/18/git/","excerpt":"介绍git是一个开源的分布式版本控制系统，用以有效、高速的处理从很小到非常大的项目版本管理。git是个工具，在linux里面也就类似gcc这样的工具一样，是一个shell命令。","text":"介绍git是一个开源的分布式版本控制系统，用以有效、高速的处理从很小到非常大的项目版本管理。git是个工具，在linux里面也就类似gcc这样的工具一样，是一个shell命令。 Git与GitHub区别Git是版本控制工具Github是一个平台，提供给用户创建git仓储空间，保存（托管）用户的一些数据文档或者代码等。 Git与CVS、SVN的区别Git是分布式版本控制系统，代码提交是在本地的（如此速度就快），当然生成补丁（patch）然后push到远程服务器上是需要联网的CVS、SVN是集中式版本控制系统，代码提交都是提交到远程服务器上CVS、SVN这样的集中式版本控制系统，它的完整代码仓库（代码仓库不仅仅只包含了代码，还包含各个历史版本的信息等）在中心服务器上，一旦这个中心服务器挂了，也就是完整的代码仓库挂了Git没有中心服务器的概念，每一个git客户端（git节点）都含有一个完整的代码仓库（前提是你之前从远程git仓库fetch过代码） 远程仓库、工作区、版本库和暂存区 远程仓库就是在github或者在gitlab上的代码。可以用git pull和git push来进行本地仓库和远程仓库的同步操作 工作区（Working Directory）从项目中取出某个版本的所有文件和目录，用以开始后续工作的叫做工作目录，也就是工作区 版本库（Repository）工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。 暂存区（Stage）暂存区就是版本库中的一个区域，具体参见上面的结构图 工作区、版本库、暂存区之间的关系使用git add把文件从工作区添加到版本库中的暂存区，git add命令可以多次用，或者使用git add file1 file2 …使用git commit提交代码，就是把暂存区的所有内容提交到当前分支需要提交的文件修改通通放到暂存区（可能有多次的git add），然后，一次性提交暂存区的所有修改到当前分支（git commit） 版本回退前提条件：已经执行git commit命令了，但是没有push到远程仓库，用以下命令可以回退其实这个回退就是将本地的HEAD指针移动到某个版本上而已，所以这个操作是非常快的。 回退到上一个版本在Git中，用HEAD表示当前版本，也就是最新的提交3628164…882e1e0，上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100版本号没必要写全，前几位就可以了，Git会自动去找。当然也不能只写前一两位，因为Git可能会找到多个版本号，就无法确定是哪一个了 12$ git reset --hard HEAD^HEAD is now at ea34578 add distributed 回退到未来的某个版本使用git log命令查看提交记录，找到需要回退版本的commit id如果在回退以后又想再次回到之前的版本，git reflog 可以查看所有分支的所有操作记录（包括commit和reset的操作），包括已经被删除的commit记录 12$ git reset --hard 3628164HEAD is now at 3628164 append GPL git log则不能查看已经删除了的commit记录 总结HEAD指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用命令git reset –hard commit_id穿梭前，用git log可以查看提交历史，以便确定要回退到哪个版本要重返未来，用git reflog查看命令历史，以便确定要回到未来的哪个版本 git reset –-soft：回退到某个版本，只回退了commit的信息，不会恢复到index file一级。如果还要提交，直接commit即可git reset -–hard：彻底回退到某个版本，本地的源码也会变为上一个版本的内容，撤销的commit中所包含的更改被冲掉 撤销修改含义命令git checkout – file意思就是，把file文件在工作区的修改全部撤销，这里有两种情况：一种是file自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；一种是file已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。总之，就是让这个文件回到最近一次git commit或git add时的状态。 场景场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout – file场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD file，就回到了场景1，第二步按场景1操作场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，版本回退一节，不过前提是没有推送到远程库 git stash场景:当你接到一个修复一个代号101的bug的任务时，你想创建一个分支issue-101来修复它，但是，当前正在dev上进行的工作还没有提交，Git还提供了一个stash功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作 git stash list查看暂存区的所有暂存修改记录 12$ git stash liststash@&#123;0&#125;: WIP on dev: 6224937 add merge git stash apply stash@{X}取出相应的暂存 git stash drop stash@{X}将记录列表中取出的对应暂存记录删除 git stash pop(推荐)取出最近一次暂存并删除记录列表中对应记录 分支管理 命令 含义 git branch 查看分支 git branch 创建分支 git checkout 切换分支 git checkout -b 创建+切换分支 git merge 合并某分支到当前分支 git branch -d 删除分支 Bug分支在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。 Feature分支添加一个新功能时，你肯定不希望因为一些实验性质的代码，把主分支搞乱了，所以，每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，最后，删除该feature分支。","categories":[{"name":"git","slug":"git","permalink":"http://yoursite.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"Redis高级知识","slug":"redis/redis","date":"2018-03-15T15:05:00.000Z","updated":"2018-03-19T16:17:31.950Z","comments":true,"path":"2018/03/15/redis/redis/","link":"","permalink":"http://yoursite.com/2018/03/15/redis/redis/","excerpt":"Redis 发布订阅Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。Redis 客户端可以订阅任意数量的频道。","text":"Redis 发布订阅Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。Redis 客户端可以订阅任意数量的频道。 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系：当有新消息通过 publish 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端： 实例以下实例演示了发布订阅是如何工作的。在我们实例中我们创建了订阅频道名为 redisChat: 123456redis 127.0.0.1:6379&gt; SUBSCRIBE redisChatReading messages... (press Ctrl-C to quit)1) \"subscribe\"2) \"redisChat\"3) (integer) 1 现在，我们先重新开启个 redis 客户端，然后在同一个频道 redisChat 发布两次消息，订阅者就能接收到消息。 123456789101112131415redis 127.0.0.1:6379&gt; publish redisChat \"Redis is a great caching technique\"(integer) 1redis 127.0.0.1:6379&gt; publish redisChat \"Learn redis by runoob.com\"(integer) 1# 订阅者的客户端会显示如下消息1) \"message\"2) \"redisChat\"3) \"Redis is a great caching technique\"1) \"message\"2) \"redisChat\"3) \"Learn redis by runoob.com\" 命令 命令 作用 PSUBSCRIBE pattern [pattern …] 订阅一个或多个符合给定模式的频道 PUBSUB subcommand [argument [argument …]] 查看订阅与发布系统状态 PUBLISH channel message 将信息发送到指定的频道 PUNSUBSCRIBE [pattern [pattern …]] 退订所有给定模式的频道 SUBSCRIBE channel [channel …] 订阅给定的一个或多个频道的信息 UNSUBSCRIBE [channel [channel …]] 指退订给定的频道 Redis 事务Redis 事务可以一次执行多个命令， 并且带有以下两个重要的保证： 批量操作在发送 EXEC 命令前被放入队列缓存。收到 EXEC 命令后进入事务执行，事务中任意命令执行失败，其余的命令依然被执行。在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中。 实例以下是一个事务的例子， 它先以 MULTI 开始一个事务， 然后将多个命令入队到事务中， 最后由 EXEC 命令触发事务， 一并执行事务中的所有命令： 12345678910111213141516171819202122redis 127.0.0.1:6379&gt; MULTIOKredis 127.0.0.1:6379&gt; SET book-name \"Mastering C++ in 21 days\"QUEUEDredis 127.0.0.1:6379&gt; GET book-nameQUEUEDredis 127.0.0.1:6379&gt; SADD tag \"C++\" \"Programming\" \"Mastering Series\"QUEUEDredis 127.0.0.1:6379&gt; SMEMBERS tagQUEUEDredis 127.0.0.1:6379&gt; EXEC1) OK2) \"Mastering C++ in 21 days\"3) (integer) 34) 1) \"Mastering Series\" 2) \"C++\" 3) \"Programming\" 单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 Redis 事务的执行并不是原子性的。事务可以理解为一个打包的批量执行脚本，但批量指令并非原子化的操作，中间某条指令的失败不会导致前面已做指令的回滚，也不会造成后续的指令不做。 命令 命令 作用 DISCARD 取消事务，放弃执行事务块内的所有命令 EXEC 执行所有事务块内的命令 MULTI 标记一个事务块的开始 UNWATCH 取消 WATCH 命令对所有 key 的监视 WATCH key [key …] 监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断 Redis 在事务失败时不进行回滚，而是继续执行余下的命令WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为 具体内容参见事物-官网文档 Redis 脚本Redis 脚本使用 Lua 解释器来执行脚本。 Reids 2.6 版本通过内嵌支持 Lua 环境。执行脚本的常用命令为 EVAL。 实例123456redis 127.0.0.1:6379&gt; EVAL \"return &#123;KEYS[1],KEYS[2],ARGV[1],ARGV[2]&#125;\" 2 key1 key2 first second1) \"key1\"2) \"key2\"3) \"first\"4) \"second\" 命令 命令 作用 EVAL script numkeys key [key …] arg [arg …] 执行 Lua 脚本 EVALSHA sha1 numkeys key [key …] arg [arg …] 执行 Lua 脚本 SCRIPT EXISTS script [script …] 查看指定的脚本是否已经被保存在缓存当中 SCRIPT FLUSH 从脚本缓存中移除所有脚本 SCRIPT KILL 杀死当前正在运行的 Lua 脚本 SCRIPT LOAD script 将脚本 script 添加到脚本缓存中，但并不立即执行这个脚本 Redis 数据备份与恢复Redis 提供了不同级别的持久化方式: RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储.AOF持久化方式记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF命令以redis协议追加保存每次写的操作到文件末尾.Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大. RDB优点RDB是一个非常紧凑的文件,它保存了某个时间点得数据集,非常适用于数据集的备份,比如你可以在每个小时报保存一下过去24小时内的数据,同时每天保存过去30天的数据,这样即使出了问题你也可以根据需求恢复到不同版本的数据集.RDB是一个紧凑的单一文件,很方便传送到另一个远端数据中心或者亚马逊的S3（可能加密），非常适用于灾难恢复.RDB在保存RDB文件时父进程唯一需要做的就是fork出一个子进程,接下来的工作全部由子进程来做，父进程不需要再做其他IO操作，所以RDB持久化方式可以最大化redis的性能.与AOF相比,在恢复大的数据集的时候，RDB方式会更快一些. 缺点如果你希望在redis意外停止工作（例如电源中断）的情况下丢失的数据最少的话，那么RDB不适合你.虽然你可以配置不同的save时间点(例如每隔5分钟并且对数据集有100个写的操作),是Redis要完整的保存整个数据集是一个比较繁重的工作,你通常会每隔5分钟或者更久做一次完整的保存,万一在Redis意外宕机,你可能会丢失几分钟的数据.RDB 需要经常fork子进程来保存数据集到硬盘上,当数据集比较大的时候,fork的过程是非常耗时的,可能会导致Redis在一些毫秒级内不能响应客户端的请求.如果数据集巨大并且CPU性能不是很好的情况下,这种情况会持续1秒,AOF也需要fork,但是你可以调节重写日志文件的频率来提高数据集的耐久度. AOF优点使用AOF 会让你的Redis更加耐久: 你可以使用不同的fsync策略：无fsync,每秒fsync,每次写的时候fsync.使用默认的每秒fsync策略,Redis的性能依然很好(fsync是由后台线程进行处理的,主线程会尽力处理客户端请求),一旦出现故障，你最多丢失1秒的数据.AOF文件是一个只进行追加的日志文件,所以不需要写入seek,即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令,你也也可使用redis-check-aof工具修复这些问题.Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 缺点对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。 命令 命令 作用 SAVE 创建当前数据库的备份,该命令将在 redis 安装目录中创建dump.rdb文件 BGSAVE 创建 redis 备份文件也可以使用命令 BGSAVE，该命令在后台执行 如果需要恢复数据，只需将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可 Redis 管道技术Redis是一种基于客户端-服务端模型以及请求/响应协议的TCP服务。这意味着通常情况下一个请求会遵循以下步骤： 客户端向服务端发送一个查询请求，并监听Socket返回，通常是以阻塞模式，等待服务端响应。服务端处理命令，并将结果返回给客户端。 更多知识详见官方文档","categories":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"Redis入门","slug":"redis/redis-introduction","date":"2018-03-15T15:05:00.000Z","updated":"2018-03-18T13:09:42.929Z","comments":true,"path":"2018/03/15/redis/redis-introduction/","link":"","permalink":"http://yoursite.com/2018/03/15/redis/redis-introduction/","excerpt":"介绍Remote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统。Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Map), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。","text":"介绍Remote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统。Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Map), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。 数据类型String（字符串）string是redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。string类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。string类型是Redis最基本的数据类型，一个键最大能存储512MB。 实例1234redis 127.0.0.1:6379&gt; SET name \"runoob\"OKredis 127.0.0.1:6379&gt; GET name\"runoob\" 在以上实例中我们使用了 Redis 的 SET 和 GET 命令。键为 name，对应的值为 runoob。注意：一个键最大能存储512MB。 Hash（哈希）Redis hash 是一个键值(key=&gt;value)对集合。Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 实例123456redis&gt; HMSET myhash field1 \"Hello\" field2 \"World\"\"OK\"redis&gt; HGET myhash field1\"Hello\"redis&gt; HGET myhash field2\"World\" 以上实例中 hash 数据类型存储了包含用户脚本信息的用户对象。 实例中我们使用了 Redis HMSET, HGETALL 命令，user:1 为键值。每个 hash 可以存储 2^32 -1 键值对（40多亿）。 List（列表）Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 实例1234567891011redis 127.0.0.1:6379&gt; lpush runoob redis(integer) 1redis 127.0.0.1:6379&gt; lpush runoob mongodb(integer) 2redis 127.0.0.1:6379&gt; lpush runoob rabitmq(integer) 3redis 127.0.0.1:6379&gt; lrange runoob 0 101) \"rabitmq\"2) \"mongodb\"3) \"redis\"redis 127.0.0.1:6379&gt; 列表最多可存储 2^32 - 1 元素 (4294967295, 每个列表可存储40多亿)。 Set（集合）Redis的Set是string类型的无序集合。集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 sadd 命令添加一个string元素到,key对应的set集合中，成功返回1,如果元素已经在集合中返回0,key对应的set不存在返回错误。 1sadd key member 实例12345678910111213redis 127.0.0.1:6379&gt; sadd runoob redis(integer) 1redis 127.0.0.1:6379&gt; sadd runoob mongodb(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 0redis 127.0.0.1:6379&gt; smembers runoob1) \"rabitmq\"2) \"mongodb\"3) \"redis\" 注意：以上实例中 rabitmq 添加了两次，但根据集合内元素的唯一性，第二次插入的元素将被忽略。集合中最大的成员数为 2^32 - 1(4294967295, 每个集合可存储40多亿个成员)。 zset(sorted set：有序集合)Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的,但分数(score)却可以重复。 zadd 命令添加元素到集合，元素在集合中存在则更新对应score 1zadd key score member 实例123456789101112redis 127.0.0.1:6379&gt; zadd runoob 0 redis(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 mongodb(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq(integer) 0redis 127.0.0.1:6379&gt; &gt; ZRANGEBYSCORE runoob 0 10001) \"mongodb\"2) \"rabitmq\"3) \"redis\"","categories":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"Kaptcha验证码介绍","slug":"captcha/kaptcha","date":"2018-03-15T15:00:00.000Z","updated":"2018-04-18T16:52:50.296Z","comments":true,"path":"2018/03/15/captcha/kaptcha/","link":"","permalink":"http://yoursite.com/2018/03/15/captcha/kaptcha/","excerpt":"kaptcha介绍kaptcha 是一个很有用的验证码生成工具。有了它，你能够生成各种样式的验证码，由于它是可配置的。kaptcha工作的原理是调用com.google.code.kaptcha.servlet.KaptchaServlet，生成一个图片。同一时候将生成的验证码字符串放到 HttpSession中。","text":"kaptcha介绍kaptcha 是一个很有用的验证码生成工具。有了它，你能够生成各种样式的验证码，由于它是可配置的。kaptcha工作的原理是调用com.google.code.kaptcha.servlet.KaptchaServlet，生成一个图片。同一时候将生成的验证码字符串放到 HttpSession中。 配置项 配置项 描述 可选值 默认值 kaptcha.border 图片边框 yes,no yes kaptcha.border.color 边框颜色 r,g,b(and optional alpha) 或者 white,black,blue black kaptcha.border.thickness 边框厚度 &gt;0 1 kaptcha.image.width 图片宽 &gt;0 200 kaptcha.image.height 图片高 &gt;0 50 kaptcha.producer.impl 图片实现类 com.google.code.kaptcha.impl.DefaultKaptcha kaptcha.textproducer.impl 文本实现类 com.google.code.kaptcha.text.impl.DefaultTextCreator kaptcha.textproducer.char.string 文本集合 abcde2345678gfynmnpwx kaptcha.textproducer.char.length 验证码长度 5 kaptcha.textproducer.font.names 字体 Arial, Courier kaptcha.textproducer.font.size 字体大小 40px kaptcha.textproducer.font.color 字体颜色 r,g,b 或者 white,black,blue black kaptcha.textproducer.char.space 文字间隔 2 kaptcha.noise.impl 干扰实现类 com.google.code.kaptcha.impl.DefaultNoise kaptcha.noise.color 干扰颜色 r,g,b 或者 white,black,blue black kaptcha.obscurificator.impl 图片样式 水纹com.google.code.kaptcha.impl.WaterRipple鱼眼com.google.code.kaptcha.impl.FishEyeGimpy阴影com.google.code.kaptcha.impl.ShadowGimpy com.google.code.kaptcha.impl.WaterRipple kaptcha.background.impl 背景实现类 com.google.code.kaptcha.impl.DefaultBackground kaptcha.background.clear.from 背景颜色渐变，开始颜色 lightGray kaptcha.background.clear.to 背景颜色渐变，结束颜色 white kaptcha.word.impl 文字渲染器 com.google.code.kaptcha.text.impl.DefaultWordRenderer kaptcha.session.key session key KAPTCHA_SESSION_KEY kaptcha.session.date session date KAPTCHA_SESSION_DATE","categories":[{"name":"captcha","slug":"captcha","permalink":"http://yoursite.com/categories/captcha/"}],"tags":[{"name":"kaptcha","slug":"kaptcha","permalink":"http://yoursite.com/tags/kaptcha/"}]},{"title":"Jcaptcha验证码介绍","slug":"captcha/jcaptcha","date":"2018-03-15T15:00:00.000Z","updated":"2018-04-18T16:58:13.460Z","comments":true,"path":"2018/03/15/captcha/jcaptcha/","link":"","permalink":"http://yoursite.com/2018/03/15/captcha/jcaptcha/","excerpt":"jcaptcha组件介绍 组件 作用 FontGenerator 设置字体随机大小范围，字体名称 BackgroundGenerator 设置背景颜色，图片大小 TextPaster 设置单词的最小最大长度，设置字的颜色，设置单词在图像中的位置是固定还是随机 ImageDeformation、ImageFilter 非必须，指定为图片变形的类，可以用变形类和过滤器两种方式 WordToImage 将FontGenerator、BackgroundGenerator、TextPaster、ImageDeformation和ImageFilter组装到一起，属于中间环节的类，类似一个容器 WordGenerator 设置取词的范围，设置一定范围，或从本地词库读取 CaptchaFactory 将配置的类，组成工程类， 也属于包装类","text":"jcaptcha组件介绍 组件 作用 FontGenerator 设置字体随机大小范围，字体名称 BackgroundGenerator 设置背景颜色，图片大小 TextPaster 设置单词的最小最大长度，设置字的颜色，设置单词在图像中的位置是固定还是随机 ImageDeformation、ImageFilter 非必须，指定为图片变形的类，可以用变形类和过滤器两种方式 WordToImage 将FontGenerator、BackgroundGenerator、TextPaster、ImageDeformation和ImageFilter组装到一起，属于中间环节的类，类似一个容器 WordGenerator 设置取词的范围，设置一定范围，或从本地词库读取 CaptchaFactory 将配置的类，组成工程类， 也属于包装类 结构图如下： 使用FontGeneratorFont类，用于设置字体的样式和字体随机的大小范围在com.octo.captcha.component.image.fontgenerator包中包含了几种Font类 Font 作用 示例 效果 RandomFontGenerator 设置随机出现的字体样式和随机的字体大小范围 RandomFontGenerator fonts = new RandomFontGenerator (new Integer(20), new Integer(30)); TwistedRandomFontGenerator 设置字体的随机大小范围，并对字体进行简单的扭曲变形，字体左右倾斜的效果 TwistedRandomFontGenerator fonts = new TwistedRandomFontGenerator(new Integer(20), new Integer(30)); TwistedAndShearedRandomFontGenerator TwistedRandomFontGenerator的子类，和 TwistedRandomFontGenerator类的构造器参数也相同，显示效果相近，采用的算法有所不同 DeformedRandomFontGenerator 构造器的参数和上两个类相同，采用的方式也类似 BackgroundGeneratorBackground设置背景颜色或背景图片、设置图片大小在com.octo.captcha.component.image.backgroundgenerator包中包含了的Background类 Background 作用 示例 效果 UniColorBackgroundGenerator 设置图片大小和以单一颜色为背景的图片背景，可以设置一个或多个备选颜色。如果想让颜色更加多变，则可以使用RandomRangeColorGenerator类，设置随机取得的RGB值，让RGB值分别的随机取得，则RGB值随机搭配的颜色将会更加多变 UniColorBackgroundGenerator background = new UniColorBackgroundGenerator(new Integer(200), new Integer(100), Color.BLUE); MultipleShapeBackgroundGenerator 设置一种多变的背景花纹 MultipleShapeBackgroundGenerator background = new MultipleShapeBackgroundGenerator (200,100,Color.RED,Color.BLUE,10,10,8,8,Color.WHITE,Color.YELLOW,3); GradientBackgroundGenerator 设置一种渐变颜色的背景，也可以设置多种颜色的渐变背景 GradientBackgroundGenerator background = new GradientBackgroundGenerator(200, 100, Color.BLACK, Color.WHITE); FunkyBackgroundGenerator 设置四种不同颜色混杂在一起的背景。如果创建过程中，给SingleColorGenerator类改为使用RandomListColorGenerator或其他类，也可以让四种颜色随机变化 SingleColorGenerator leftUpColor = new SingleColorGenerator(Color.RED); SingleColorGenerator leftDownColor = new SingleColorGenerator(Color.YELLOW); SingleColorGenerator rightUpColor = new SingleColorGenerator(Color.BLUE); SingleColorGenerator rightDownColor = new SingleColorGenerator(Color.GREEN); FunkyBackgroundGenerator background = new FunkyBackgroundGenerator( 200, 100, leftUpColor, leftDownColor, rightUpColor, rightDownColor, 0.5f); FileReaderRandomBackgroundGenerator 设置从指定目录读取图片作为生成图片的背景，目录中的图片，会在类构造期间就全部读取到内存中 FileReaderRandomBackgroundGenerator background = new FileReaderRandomBackgroundGenerator(200, 100, “E:¥¥testproject¥¥jcaptcha¥¥images¥¥backgrounds”); EllipseBackgroundGenerator 一种花样的背景，该类到目前为止没有更多的配置选项，只能设置图片大小，颜色和其他相关的值都不能设置 TextPasterTextPaster可以设置单词的最小和最大长度和设置字的颜色（前景颜色），设置单词在图像中的位置是固定还是随机 在com.octo.captcha.component.image.textpaster包中包含了的常用TextPaster类 TextPaster 作用 示例 效果 RandomTextPaster 最常用的TextPaster，可以设置生成单词的随机长度范围，和设置一种或多种前景颜色，生成的字符串在图像中的位置是随机的 RandomTextPaster textPaster = new RandomTextPaster(new Integer(5), new Integer(10), Color.BLACK);也可以设置多种随机前景颜色：Color[] colors = new Color[]{Color.RED,Color.YELLOW,Color.BLUE}; RandomListColorGenerator randomColors = new RandomListColorGenerator(colors); RandomTextPaster textPaster = new RandomTextPaster(new Integer(5), new Integer(10), randomColors); SimpleTextPaster 和RandomTextPaster用法和参数都相同，但是生成的字符串位置不变 NonLinearTextPaster 设置非直线的TextPaster，最后生成的文本成非直线的排列 NonLinearTextPaster textPaster = new NonLinearTextPaster (new Integer(5), new Integer(10), Color.BLACK); DoubleTextPaster 可以形成两行好像重影效果的文本，单词位置固定 DoubleTextPaster textPaster = new DoubleTextPaster(new Integer(5),new Integer(10), Color.BLACK); DoubleRandomTextPaster 和DoubleTextPaster类类似的效果，但是单词在图像中出现的位置和第二行字上下位置，是随机的 DecoratedRandomTextPaster 为生成的文本加上修饰，例如干扰线、干扰点 SingleColorGenerator lineColor = new SingleColorGenerator(Color.BLUE); TextDecorator decorator1 = new LineTextDecorator(new Integer(1), lineColor, AlphaComposite.SRC_OVER); TextDecorator[] decorators = new TextDecorator[] { decorator1 }; SingleColorGenerator color = new SingleColorGenerator(Color.RED); DecoratedRandomTextPaster textPaster = new DecoratedRandomTextPaster(new Integer(5), new Integer(10), color, decorators);LineTextDecorator的构造器的最后一个参数是干扰线的类型，在该参数上进行修改，也能取得不错的效果，例如将AlphaComposite.SRC_OVER改为AlphaComposite.CLEAR、AlphaComposite.SRC_IN、AlphaComposite.XORLineTextDecorator类是线型的装饰器，Captcha还提供了一种点状的装饰类：SingleColorGenerator dColor = new SingleColorGenerator(Color.BLUE); TextDecorator decorator2 = new BaffleTextDecorator(new Integer(1), dColor, AlphaComposite.SRC_OUT); TextDecorator[] decorators = new TextDecorator[] { decorator2 }; SingleColorGenerator color = new SingleColorGenerator(Color.RED); DecoratedRandomTextPaster textPaster = new DecoratedRandomTextPaster(new Integer(5), new Integer(10), color, decorators); Deformation和ImageFilter&nbsp;&nbsp;&nbsp;&nbsp;这两种类，都提供了对图片字体进行进一步变形的功能，Deformation类是Captcha框架中提供的变形类 和接口，ImageFilter是一个使用java提供图形图像方面服务的公司提供的通用的图形变形过滤器，可以实现的效果很多。&nbsp;&nbsp;&nbsp;&nbsp;Deformation和ImageFilter都分为3层，一种是在文字级别的，一种是在背景级别的，还有在整个图片级别的，用于在图片生成不同的阶段对图片进行变形。&nbsp;&nbsp;&nbsp;&nbsp;Deformation类，当前Captcha只提供了一种选择，就是PuzzleImageDeformation类，该类的效果是将 图片旋转，然后直接贴在原来图片的上面，所以效果不是很好，相信以后Captcha会提供更多的Deformation类，并且会不断完善，但是目前，不推荐使用。&nbsp;&nbsp;&nbsp;&nbsp;ImageFilter过滤器，有很多现有的资源可以供选择使用，是目前最好的选择。 12345678910111213141516171819202122232425// font RandomFontGenerator fonts = new RandomFontGenerator(new Integer(20), new Integer(20)); // background BackgroundGenerator background = new UniColorBackgroundGenerator(new Integer(400), new Integer(300), Color.white); // textPaster RandomTextPaster textPaster = new RandomTextPaster(new Integer(5), new Integer(5), Color.BLACK); // no deformation ImageDeformation noneDeformation = new ImageDeformationByFilters( new ImageFilter[] &#123;&#125;);// filter创建过滤器 ImageDeformation filters = new ImageDeformationByFilters( new ImageFilter[] &#123; new TwirlFilter() &#125;); DeformedComposedWordToImage cwti = new DeformedComposedWordToImage(fonts, background, textPaster, noneDeformation, filters, noneDeformation); // 设置随机取词范围RandomWordGenerator words = new RandomWordGenerator(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\");GimpyFactory gimpy = new GimpyFactory(words, cwti); SimpleListImageCaptchaEngine engine = new SimpleListImageCaptchaEngine();engine.setFactories(new CaptchaFactory[] &#123; gimpy &#125;); FastHashMapCaptchaStore captchaStore = new FastHashMapCaptchaStore();// 创建Captcha服务 DefaultManageableImageCaptchaService defaultService = new DefaultManageableImageCaptchaService(captchaStore, engine, 180, 100000, 75000); return defaultService; &nbsp;&nbsp;&nbsp;&nbsp;上述的代码，是在创建一个Captcha服务器的过程中，将过滤器加入其中，其中使用的font、background、 和testPaster类，都使用了最简单的，没有变形效果的类，意在只查看过滤器对整个图片的映像。 更多过滤器效果，可以参见Java Image Filters","categories":[{"name":"captcha","slug":"captcha","permalink":"http://yoursite.com/categories/captcha/"}],"tags":[{"name":"jcaptcha","slug":"jcaptcha","permalink":"http://yoursite.com/tags/jcaptcha/"}]},{"title":"Kafka知识探索","slug":"kafka/kafka","date":"2018-03-11T03:08:00.000Z","updated":"2018-03-12T15:07:23.175Z","comments":true,"path":"2018/03/11/kafka/kafka/","link":"","permalink":"http://yoursite.com/2018/03/11/kafka/kafka/","excerpt":"环境搭建(Linux)1. Kafka下载 下载地址：http://kafka.apache.org/downloads wget http://apache.fayea.com/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz tar -xvf kafka_2.11-0.10.1.0.tgzcd kafka_2.11-0.10.1.0 2. Zookeeper安装Kafka需要Zookeeper的监控，所以先要安装Zookeeper，如何安装请传送至： hadoop、 zookeeper、 hbase、spark集群环境搭建 ，安装完成以后依次启动各个节点","text":"环境搭建(Linux)1. Kafka下载 下载地址：http://kafka.apache.org/downloads wget http://apache.fayea.com/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz tar -xvf kafka_2.11-0.10.1.0.tgzcd kafka_2.11-0.10.1.0 2. Zookeeper安装Kafka需要Zookeeper的监控，所以先要安装Zookeeper，如何安装请传送至： hadoop、 zookeeper、 hbase、spark集群环境搭建 ，安装完成以后依次启动各个节点 3. 配置kafka broker集群 首先把Kafka解压后的目录复制到集群的各台服务器 然后修改各个服务器的配置文件：进入Kafka的config目录，修改server.properties12345678910# brokerid就是指各台服务器对应的id，所以各台服务器值不同broker.id=0# 端口号，无需改变port=9092# 当前服务器的IP，各台服务器值不同host.name=192.168.0.10# Zookeeper集群的ip和端口号zookeeper.connect=192.168.0.10:2181,192.168.0.11:2181,192.168.0.12:2181# 日志目录log.dirs=/home/www/kafka-logs 4. 启动Kafka 在每台服务器上进入Kafka目录，分别执行以下命令：1bin/kafka-server-start.sh config/server.properties &amp; 5. Kafka常用命令 5.1 新建topic1bin/kafka-topics.sh --create --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --replication-factor 2 --partitions 2 --topic test test有两个复制因子和两个分区 5.2 查看某个topic主题1bin/kafka-topics.sh --describe --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --topic test 其中第一行是所有分区的信息，下面的每一行对应一个分区Leader：负责某个分区所有读写操作的节点Replicas：复制因子节点Isr：存活节点 5.3 查看Kafka所有的主题1bin/kafka-topics.sh --list --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 5.4 终端发送消息1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 5.5 终端接收（消费）消息1bin/kafka-console-consumer.sh --zookeeper hxf:2181,cfg:2181,jqs:2181,jxf:2181,sxtb:2181 --bootstrap-server localhost:9092 --topic test --from-beginning 简介1. 基本术语消息在Kafka中，每一个消息由键、值和一个时间戳组成 主题和日志 Kafka集群存储同一类别的消息流称为主题 主题会有多个订阅者（0个1个或多个），当主题发布消息时，会向订阅者推送记录 针对每一个主题，Kafka集群维护了一个像下面这样的分区日志： 这些分区位于不同的服务器上，每一个分区可以看做是一个结构化的提交日志，每写入一条记录都会记录到其中一个分区并且分配一个唯一地标识其位置的数字称为偏移量offset Kafka集群会将发布的消息保存一段时间，不管是否被消费。例如，如果设置保存天数为2天，那么从消息发布起的两天之内，该消息一直可以被消费，但是超过两天后就会被丢弃以节省空间。其次，Kafka的数据持久化性能很好，所以长时间存储数据不是问题如下图所示，生产者每发布一条消息就会向分区log写入一条记录的offset，而消费者就是通过offset来读取对应的消息的，一般来说每读取一条消息，消费者对应要读取的offset就加1，例如最后一条读到offset=12，那么下条offset就为13.由于消费者通过offset来读取消息，所以可以重复读取已经读过的记录，或者跳过某些记录不读Kafka中采用分区的设计有几个目的。一是可以处理更多的消息，不受单台服务器的限制。Topic拥有多个分区意味着它可以不受限的处理更多的数据。第二，分区可以作为并行处理的单元 分布式Log的分区被分布到集群中的多个服务器上。每个服务器处理它分到的分区。 根据配置每个分区还可以复制到其它服务器作为备份容错每个分区有一个leader，零或多个follower。Leader处理此分区的所有的读写请求，而follower被动的复制数据。如果leader宕机，其它的一个follower会被推举为新的leader。 一台服务器可能同时是一个分区的leader，另一个分区的follower。 这样可以平衡负载，避免所有的请求都只让一台或者某几台服务器处理 生产者生产者往某个Topic上发布消息。生产者还可以选择将消息分配到Topic的哪个节点上。最简单的方式是轮询分配到各个分区以平衡负载，也可以根据某种算法依照权重选择分区 消费者Kafka有一个消费者组的概念，生产者把消息发到的是消费者组，在消费者组里面可以有很多个消费者实例，如下图所示： Kafka集群有两台服务器，四个分区，此外有两个消费者组A和B，消费者组A具有2个消费者实例C1-2，消费者B具有4个消费者实例C3-6那么Kafka发送消息的过程是怎样的呢？例如此时我们创建了一个主题test，有两个分区，分别是Server1的P0和Server2的P1，假设此时我们通过test发布了一条消息，那么这条消息是发到P0还是P1呢，或者是都发呢？答案是只会发到P0或P1其中之一，也就是消息只会发给其中的一个分区分区接收到消息后会记录在分区日志中，记录的方式我们讲过了，就是通过offset，正因为有这个偏移量的存在，所以一个分区内的消息是有先后顺序的，即offset大的消息比offset小的消息后到。但是注意，由于消息随机发往主题的任意一个分区，因此虽然同一个分区的消息有先后顺序，但是不同分区之间的消息就没有先后顺序了，那么如果我们要求消费者顺序消费主题发的消息那该怎么办呢，此时只要在创建主题的时候只提供一个分区即可讲完了主题发消息，接下来就该消费者消费消息了，假设上面test的消息发给了分区P0，此时从图中可以看到，有两个消费者组，那么P0将会把消息发到哪个消费者组呢？从图中可以看到，P0把消息既发给了消费者组A也发给了B，但是A中消息仅被C1消费，B中消息仅被C3消费。这就是我们要讲的，主题发出的消息会发往所有的消费者组，而每一个消费者组下面可以有很多消费者实例，这条消息只会被他们中的一个消费掉 2. 核心APIKafka具有4个核心API： Producer API：用于向Kafka主题发送消息。 Consumer API：用于从订阅主题接收消息并且处理这些消息。 Streams API：作为一个流处理器，用于从一个或者多个主题中消费消息流然后为其他主题生产消息流，高效地将输入流转换为输出流。 Connector API：用于构建和运行将Kafka主题和已有应用或者数据系统连接起来的可复用的生产者或消费者。例如一个主题到一个关系型数据库的连接能够捕获表的任意变化。 3. 应用场景Kafka用作消息系统Kafka流的概念与传统企业消息系统有什么异同？传统消息系统有两个模型：队列和发布-订阅系统。在队列模式中，每条服务器的消息会被消费者池中的一个所读取；而发布-订阅系统中消息会广播给所有的消费者。这两种模式各有优劣。队列模式的优势是可以将消息数据让多个消费者处理以实现程序的可扩展，然而这就导致其没有多个订阅者，只能用于一个进程。发布-订阅模式的好处在于数据可以被多个进程消费使用，但是却无法使单一程序扩展性能Kafka中消费者组的概念同时涵盖了这两方面。对应于队列的概念，Kafka中每个消费者组中有多个消费者实例可以接收消息；对应于发布-订阅模式，Kafka中可以指定多个消费者组来订阅消息相对传统消息系统，Kafka可以提供更强的顺序保证 Kafka用作存储系统任何发布消息与消费消息解耦的消息队列其实都可以看做是用来存放发布的消息的存储系统，而Kafka是一个非常高效的存储系统写入Kafka的数据会被存入磁盘并且复制到集群中以容错。Kafka允许生产者等待数据完全复制并且确保持久化到磁盘的确认应答Kafka使用的磁盘结构扩容性能很好——不管服务器上有50KB还是50TB，Kafka的表现都是一样的由于能够精致的存储并且供客户端程序进行读操作，你可以把Kafka看做是一个用于高性能、低延迟的存储提交日志、复制及传播的分布式文件系统 Kafka的流处理仅仅读、写、存储流数据是不够的，Kafka的目的是实现实时流处理。在Kafka中一个流处理器的处理流程是首先持续性的从输入主题中获取数据流，然后对其进行一些处理，再持续性地向输出主题中生产数据流。例如一个销售商应用，接收销售和发货量的输入流，输出新订单和调整后价格的输出流可以直接使用producer和consumer API进行简单的处理。对于复杂的转换，Kafka提供了更强大的Streams API。可构建聚合计算或连接流到一起的复杂应用程序流处理有助于解决这类应用面临的硬性问题：处理无序数据、代码更改的再处理、执行状态计算等Streams API所依托的都是Kafka的核心内容：使用producer和consumer API作为输入，使用Kafka作为状态存储，在流处理实例上使用相同的组机制来实现容错 使用消费者自动提交使用如下api自动提交： 1properties.put(\"enable.auto.commit\", \"false\"); 消费者手动提交每个消费者和对应的patition建立对应的流来读取kafka上面的数据，如果comsumer得到数据，那么kafka就会自动去维护该comsumer的offset，例如在获取到kafka的消息后正准备入库（未入库），但是消费者挂了，那么如果让kafka自动去维护offset，它就会认为这条数据已经被消费了，那么会造成数据丢失。但是kafka可以让你自己去手动提交，如果在上面的场景中，那么需要我们手动commit，如果comsumer挂了 那么程序就不会执行commit这样的话 其他同group的消费者又可以消费这条数据，保证数据不丢，先要做如下设置： 12//设置不自动提交，自己手动更新offsetproperties.put(\"enable.auto.commit\", \"false\"); 使用如下api提交： 1consumer.commitSync();","categories":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"ELK环境搭建","slug":"ELK","date":"2017-08-04T15:05:00.000Z","updated":"2018-03-11T04:27:17.095Z","comments":true,"path":"2017/08/04/ELK/","link":"","permalink":"http://yoursite.com/2017/08/04/ELK/","excerpt":"一、配置系统： Windows 8.1 elasticsearch：5.5.1 logstash：2.0.0 kibana：5.5.1 注：由于实验性搭建，选择windows系统，但选择Linux系统效果更佳 二、部署方案1.ELK+Redis2.ELK+Kafka 注：本次搭建选用第一种方案","text":"一、配置系统： Windows 8.1 elasticsearch：5.5.1 logstash：2.0.0 kibana：5.5.1 注：由于实验性搭建，选择windows系统，但选择Linux系统效果更佳 二、部署方案1.ELK+Redis2.ELK+Kafka 注：本次搭建选用第一种方案 三、安装 前提:下载nssm 1. Elasticsearch下载： download2. logstash下载： download3. kibana下载： download注册为windows服务(a) 将下载的nssm.exe分别拷贝到Elasticsearch、logstash和kibana解压后的bin目录下，然后CMD进入bin执行nssm install 服务名,例如Elasticsearch 的执行nssm install elasticsearch-service..(b) 分析选择path为各压缩包的bin目录下的elasticsearch.bat、logstash.bat和kibana.bat(c) Details选项卡设置显示名为Windows名(d) 最后选择Install service四、部署1. 创建Maven项目elk-log(可另外取名)，pom文件为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.suncj&lt;/groupId&gt; &lt;artifactId&gt;elk-log&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;elk-log&lt;/name&gt; &lt;description&gt;elk日志生成项目&lt;/description&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;4.2.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty.aggregate&lt;/groupId&gt; &lt;artifactId&gt;jetty-all&lt;/artifactId&gt; &lt;version&gt;8.1.19.v20160209&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;4.9&lt;/version&gt; &lt;/dependency&gt; &lt;!--实现slf4j接口并整合 --&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2. 配置logback,logback.xml文件为：1234567891011121314151617181920212223242526&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration debug=\"false\"&gt; &lt;appender name=\"console\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt; &lt;!-- 格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符 --&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %c&#123;1&#125;.%M:%L - %m%n &lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=\"stash\" class=\"net.logstash.logback.appender.LogstashTcpSocketAppender\"&gt; &lt;destination&gt;127.0.0.1:9250&lt;/destination&gt; &lt;encoder charset=\"UTF-8\" class=\"net.logstash.logback.encoder.LogstashEncoder\" /&gt; &lt;/appender&gt; &lt;logger name=\"com.suncj\" level=\"INFO\" /&gt; &lt;root level=\"INFO\"&gt; &lt;appender-ref ref=\"console\" /&gt; &lt;appender-ref ref=\"stash\" /&gt; &lt;/root&gt;&lt;/configuration&gt; 3.设置项目定时任务(打日志)定时任务类LogProducer:12345678910111213141516171819202122232425package com.suncj.elk;import java.util.Random;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * 日志生成器&lt;br&gt; * 版权：Copyright (c) 2015-2016&lt;br&gt; * 创建日期：2017年8月5日&lt;br&gt; */public class LogProducer &#123; private static final Logger log = LoggerFactory.getLogger(LogProducer.class); private Random rand = new Random(); private static int logId = 0; public void produce() &#123; log.info(\"log_id: &#123;&#125; , content:&#123;&#125;\", logId, String.format(\"I am %s\", logId + rand.nextInt(100000))); logId++; &#125;&#125; 项目启动类:123456789101112131415161718192021222324package com.suncj.elk;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Application &#123; private static Logger logger = LoggerFactory.getLogger(Application.class); public static ApplicationContext appContext; public static void main(String[] args) &#123; try &#123; logger.info(\"准备加载程序\"); appContext = new ClassPathXmlApplicationContext(\"app-*.xml\"); logger.info(\"加载完成\"); &#125; catch (Exception e) &#123; logger.error(\"主程序出错:\", e); &#125; &#125;&#125; 其他配置文件：app-task.xml12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:util=\"http://www.springframework.org/schema/util\" xmlns:task=\"http://www.springframework.org/schema/task\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;bean id=\"logProducer\" class=\"com.suncj.elk.LogProducer\"&gt;&lt;/bean&gt; &lt;task:scheduled-tasks&gt; &lt;task:scheduled ref=\"logProducer\" method=\"produce\" cron=\"0/5 * * * * *\" /&gt; &lt;/task:scheduled-tasks&gt;&lt;/beans&gt; 2. logstash配置(a) run_es.bat，run_redis.bat1logstash.bat agent -f logstash_es.conf (b) logstash_redis.conf1234567891011121314151617input &#123; tcp &#123; host =&gt; \"127.0.0.1\" port =&gt; 9250 mode =&gt; \"server\" codec =&gt; json_lines &#125;&#125;output &#123; redis &#123; host =&gt; \"127.0.0.1\" port =&gt; 6379 db =&gt; 1 data_type =&gt; \"list\" key =&gt; \"log:es\" &#125;&#125; (c) logstash_es.conf12345678910111213141516171819input &#123; redis &#123; data_type =&gt; \"list\" key =&gt; \"log:es\" host =&gt; \"127.0.0.1\" db =&gt; 1 port =&gt; 6379 &#125;&#125;output &#123; stdout&#123; codec =&gt; rubydebug &#125; elasticsearch &#123; hosts =&gt; [\"127.0.0.1:9200\"] index =&gt; \"log-es-%&#123;+YYYY.MM.dd&#125;\" flush_size =&gt; 1000 &#125;&#125; 注: logstash注册为windows服务时需要创建两个bat文件，一个用于项目日志存储到redis；另外一个用户读取redis,输出到elasticsearch，因此需要注册两个服务名不同的windows服务 参考资料https://kibana.logstash.es/content/kibana/index.html http://blog.csdn.net/tulizi/article/details/52972824 http://udn.yyuap.com/doc/logstash-best-practice-cn/input/redis.html https://www.elastic.co/guide/en/logstash/current/codec-plugins.html","categories":[{"name":"ELK","slug":"ELK","permalink":"http://yoursite.com/categories/ELK/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://yoursite.com/tags/ELK/"}]},{"title":"Spring-Shiro介绍及其使用","slug":"spring/spring-shiro","date":"2017-08-01T03:17:11.000Z","updated":"2017-08-07T16:46:29.570Z","comments":true,"path":"2017/08/01/spring/spring-shiro/","link":"","permalink":"http://yoursite.com/2017/08/01/spring/spring-shiro/","excerpt":"What is Apache Shiro?Apache Shiro是一个功能强大、灵活的，开源的安全框架。它可以干净利落地处理身份验证、授权、企业会话管理和加密。 Apache Shiro的首要目标是易于使用和理解。安全通常很复杂，甚至让人感到很痛苦，但是Shiro却不是这样子的。一个好的安全框架应该屏蔽复杂性，向外暴露简单、直观的API，来简化开发人员实现应用程序安全所花费的时间和精力。","text":"What is Apache Shiro?Apache Shiro是一个功能强大、灵活的，开源的安全框架。它可以干净利落地处理身份验证、授权、企业会话管理和加密。 Apache Shiro的首要目标是易于使用和理解。安全通常很复杂，甚至让人感到很痛苦，但是Shiro却不是这样子的。一个好的安全框架应该屏蔽复杂性，向外暴露简单、直观的API，来简化开发人员实现应用程序安全所花费的时间和精力。Shiro能做什么呢？ 验证用户身份 用户访问权限控制，比如：1、判断用户是否分配了一定的安全角色。2、判断用户是否被授予完成某个操作的权限 在非 web 或 EJB 容器的环境下可以任意使用Session API 可以响应认证、访问控制，或者 Session 生命周期中发生的事件 可将一个或以上用户安全数据源数据组合成一个复合的用户 “view”(视图) 支持单点登录(SSO)功能 支持提供“Remember Me”服务，获取用户关联信息而无需登录… 等等——都集成到一个有凝聚力的易于使用的API。 Shiro 致力在所有应用环境下实现上述功能，小到命令行应用程序，大到企业应用中，而且不需要借助第三方框架、容器、应用服务器等。当然 Shiro 的目的是尽量的融入到这样的应用环境中去，但也可以在它们之外的任何环境下开箱即用。 Apache Shiro Features 特性Apache Shiro是一个全面的、蕴含丰富功能的安全框架。下图为描述Shiro功能的框架图： Authentication（认证）, Authorization（授权）, Session Management（会话管理）, Cryptography（加密）被 Shiro 框架的开发团队称之为应用安全的四大基石。那么就让我们来看看它们吧： Authentication（认证）：用户身份识别，通常被称为用户“登录” Authorization（授权）：访问控制。比如某个用户是否具有某个操作的使用权限。 Session Management（会话管理）：特定于用户的会话管理,甚至在非web 或 EJB 应用程序。 Cryptography（加密）：在对数据源使用加密算法加密的同时，保证易于使用。 还有其他的功能来支持和加强这些不同应用环境下安全领域的关注点。特别是对以下的功能支持： Web支持：Shiro 提供的 web 支持 api ，可以很轻松的保护 web 应用程序的安全。 缓存：缓存是 Apache Shiro 保证安全操作快速、高效的重要手段。 并发：Apache Shiro 支持多线程应用程序的并发特性。 测试：支持单元测试和集成测试，确保代码和预想的一样安全。 “Run As”：这个功能允许用户假设另一个用户的身份(在许可的前提下)。 “Remember Me”：跨 session 记录用户的身份，只有在强制需要时才需要登录。 注意： Shiro不会去维护用户、维护权限，这些需要我们自己去设计/提供，然后通过相应的接口注入给Shiro High-Level Overview 高级概述在概念层，Shiro 架构包含三个主要的理念：Subject,SecurityManager和 Realm。下面的图展示了这些组件如何相互作用，我们将在下面依次对其进行描述。 Subject：当前用户，Subject 可以是一个人，但也可以是第三方服务、守护进程帐户、时钟守护任务或者其它–当前和软件交互的任何事件。 SecurityManager：管理所有Subject，SecurityManager 是 Shiro 架构的核心，配合内部安全组件共同组成安全伞。 Realms：用于进行权限信息的验证，我们自己实现。Realm 本质上是一个特定的安全 DAO：它封装与数据源连接的细节，得到Shiro 所需的相关的数据。在配置 Shiro 的时候，你必须指定至少一个Realm 来实现认证（authentication）和/或授权（authorization）。 我们需要实现Realms的Authentication 和 Authorization。其中 Authentication 是用来验证用户身份，Authorization 是授权访问控制，用于对用户进行的操作授权，证明该用户是否允许进行当前操作，如访问某个链接，某个资源文件等。 快速上手pom.xml1234567891011121314151617181920212223242526272829&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sourceforge.nekohtml&lt;/groupId&gt; &lt;artifactId&gt;nekohtml&lt;/artifactId&gt; &lt;version&gt;1.9.22&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Shiro 配置1234567891011121314151617181920212223242526272829303132333435363738394041@Configurationpublic class ShiroConfig &#123; @Bean public ShiroFilterFactoryBean shirFilter(SecurityManager securityManager) &#123; System.out.println(\"ShiroConfiguration.shirFilter()\"); ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); shiroFilterFactoryBean.setSecurityManager(securityManager); //拦截器. Map&lt;String,String&gt; filterChainDefinitionMap = new LinkedHashMap&lt;String,String&gt;(); // 配置不会被拦截的链接 顺序判断 filterChainDefinitionMap.put(\"/static/**\", \"anon\"); //配置退出 过滤器,其中的具体的退出代码Shiro已经替我们实现了 filterChainDefinitionMap.put(\"/logout\", \"logout\"); //&lt;!-- 过滤链定义，从上向下顺序执行，一般将/**放在最为下边 --&gt;:这是一个坑呢，一不小心代码就不好使了; //&lt;!-- authc:所有url都必须认证通过才可以访问; anon:所有url都都可以匿名访问--&gt; filterChainDefinitionMap.put(\"/**\", \"authc\"); // 如果不设置默认会自动寻找Web工程根目录下的\"/login.jsp\"页面 shiroFilterFactoryBean.setLoginUrl(\"/login\"); // 登录成功后要跳转的链接 shiroFilterFactoryBean.setSuccessUrl(\"/index\"); //未授权界面; shiroFilterFactoryBean.setUnauthorizedUrl(\"/403\"); shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap); return shiroFilterFactoryBean; &#125; @Bean public MyShiroRealm myShiroRealm()&#123; MyShiroRealm myShiroRealm = new MyShiroRealm(); return myShiroRealm; &#125; @Bean public SecurityManager securityManager()&#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); securityManager.setRealm(myShiroRealm()); return securityManager; &#125;&#125; Filter Chain定义说明： 1、一个URL可以配置多个Filter，使用逗号分隔 2、当设置多个过滤器时，全部验证通过，才视为通过 3、部分过滤器可指定参数，如perms，roles Shiro内置的FilterChain: Filter Name Class anon org.apache.shiro.web.filter.authc.AnonymousFilter authc org.apache.shiro.web.filter.authc.FormAuthenticationFilter authcBasic org.apache.shiro.web.filter.authc.BasicHttpAuthenticationFilter perms org.apache.shiro.web.filter.authz.PermissionsAuthorizationFilter port org.apache.shiro.web.filter.authz.PortFilter rest org.apache.shiro.web.filter.authz.HttpMethodPermissionFilter roles org.apache.shiro.web.filter.authz.RolesAuthorizationFilter ssl org.apache.shiro.web.filter.authz.SslFilter user org.apache.shiro.web.filter.authc.UserFilter anon:所有url都都可以匿名访问 authc: 需要认证才能进行访问 user:配置记住我或认证通过可以访问 登录认证实现 在认证、授权内部实现机制中都有提到，最终处理都将交给Real进行处理。因为在Shiro中，最终是通过Realm来获取应用程序中的用户、角色及权限信息的。通常情况下，在Realm中会直接从我们的数据源中获取Shiro需要的验证信息。可以说，Realm是专用于安全框架的DAO.Shiro的认证过程最终会交由Realm执行，这时会调用Realm的getAuthenticationInfo(token)方法。 该方法主要执行以下操作: 1、检查提交的进行认证的令牌信息 2、根据令牌信息从数据源(通常为数据库)中获取用户信息 3、对用户信息进行匹配验证。 4、验证通过将返回一个封装了用户信息的AuthenticationInfo实例。 5、验证失败则抛出AuthenticationException异常信息。 而在我们的应用程序中要做的就是自定义一个Realm类，继承AuthorizingRealm抽象类，重载doGetAuthenticationInfo()，重写获取用户信息的方法。 doGetAuthenticationInfo的重写 12345678910111213141516171819202122@Overrideprotected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123; System.out.println(\"MyShiroRealm.doGetAuthenticationInfo()\"); //获取用户的输入的账号. String username = (String)token.getPrincipal(); System.out.println(token.getCredentials()); //通过username从数据库中查找 User对象，如果找到，没找到. //实际项目中，这里可以根据实际情况做缓存，如果不做，Shiro自己也是有时间间隔机制，2分钟内不会重复执行该方法 UserInfo userInfo = userInfoService.findByUsername(username); System.out.println(\"-----&gt;&gt;userInfo=\"+userInfo); if(userInfo == null)&#123; return null; &#125; SimpleAuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo( userInfo, //用户名 userInfo.getPassword(), //密码 ByteSource.Util.bytes(userInfo.getCredentialsSalt()),//salt=username+salt getName() //realm name ); return authenticationInfo;&#125; 链接权限的实现 shiro的权限授权是通过继承AuthorizingRealm抽象类，重载doGetAuthorizationInfo();当访问到页面的时候，链接配置了相应的权限或者shiro标签才会执行此方法否则不会执行，所以如果只是简单的身份认证没有权限的控制的话，那么这个方法可以不进行实现，直接返回null即可。在这个方法中主要是使用类：SimpleAuthorizationInfo进行角色的添加和权限的添加。 12345678910111213@Overrideprotected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; System.out.println(\"权限配置--&gt;MyShiroRealm.doGetAuthorizationInfo()\"); SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo(); UserInfo userInfo = (UserInfo)principals.getPrimaryPrincipal(); for(SysRole role:userInfo.getRoleList())&#123; authorizationInfo.addRole(role.getRole()); for(SysPermission p:role.getPermissions())&#123; authorizationInfo.addStringPermission(p.getPermission()); &#125; &#125; return authorizationInfo;&#125; 当然也可以添加set集合：roles是从数据库查询的当前用户的角色，stringPermissions是从数据库查询的当前用户对应的权限 12authorizationInfo.setRoles(roles);authorizationInfo.setStringPermissions(stringPermissions); 就是说如果在shiro配置文件中添加了filterChainDefinitionMap.put(“/add”, “perms[权限添加]”);就说明访问/add这个链接必须要有“权限添加”这个权限才可以访问，如果在shiro配置文件中添加了filterChainDefinitionMap.put(“/add”, “roles[100002]，perms[权限添加]”);就说明访问/add这个链接必须要有“权限添加”这个权限和具有“100002”这个角色才可以访问。 参考附录:Apache Shiro中文手册 Spring Boot Shiro权限管理【从零开始学Spring Boot】SpringBoot+shiro整合学习之登录认证和权限控制 springboot整合shiro-登录认证和权限管理","categories":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"},{"name":"shiro","slug":"shiro","permalink":"http://yoursite.com/tags/shiro/"}]},{"title":"ThreadLocal","slug":"java/thread-local","date":"2017-07-22T10:54:25.000Z","updated":"2018-07-24T17:15:49.944Z","comments":true,"path":"2017/07/22/java/thread-local/","link":"","permalink":"http://yoursite.com/2017/07/22/java/thread-local/","excerpt":"ThreadLocal用法Java中线程的同步机制保证了多线程访问共享变量的安全性，通常我们使用synchronized关键字来实现。在多个线程对共享变量进行读写操作时，同步锁保证了同一时间只有一个线程对共享变量进行操作，概括地说，这是一种“以时间换空间”的解决策略。 在JDK1.2中引入了ThreadLocal类来提供了一种“以空间换时间”的同步解决策略。ThreadLocal内部维护了一份类似Map的静态变量ThreadLocalMap，其中key为当前线程，value为共享变量。JDK1.5引入泛型，ThreadLocal也同时支持泛型。","text":"ThreadLocal用法Java中线程的同步机制保证了多线程访问共享变量的安全性，通常我们使用synchronized关键字来实现。在多个线程对共享变量进行读写操作时，同步锁保证了同一时间只有一个线程对共享变量进行操作，概括地说，这是一种“以时间换空间”的解决策略。 在JDK1.2中引入了ThreadLocal类来提供了一种“以空间换时间”的同步解决策略。ThreadLocal内部维护了一份类似Map的静态变量ThreadLocalMap，其中key为当前线程，value为共享变量。JDK1.5引入泛型，ThreadLocal也同时支持泛型。其具体实现如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124public class ThreadLocal&lt;T&gt; &#123; /** * ThreadLocals rely on per-thread hash maps attached to each thread * (Thread.threadLocals and inheritableThreadLocals). The ThreadLocal * objects act as keys, searched via threadLocalHashCode. This is a * custom hash code (useful only within ThreadLocalMaps) that eliminates * collisions in the common case where consecutively constructed * ThreadLocals are used by the same threads, while remaining well-behaved * in less common cases. */ private final int threadLocalHashCode = nextHashCode(); /** * The next hash code to be given out. Accessed only by like-named method. */ private static int nextHashCode = 0; /** * The difference between successively generated hash codes - turns * implicit sequential thread-local IDs into near-optimally spread * multiplicative hash values for power-of-two-sized tables. */ private static final int HASH_INCREMENT = 0x61c88647; /** * Compute the next hash code. The static synchronization used here * should not be a performance bottleneck. When ThreadLocals are * generated in different threads at a fast enough rate to regularly * contend on this lock, memory contention is by far a more serious * problem than lock contention. */ private static synchronized int nextHashCode() &#123; int h = nextHashCode; nextHashCode = h + HASH_INCREMENT; return h; &#125; /** * Creates a thread local variable. */ public ThreadLocal() &#123; &#125; /** * Returns the value in the current thread's copy of this thread-local * variable. Creates and initializes the copy if this is the first time * the thread has called this method. * * @return the current thread's value of this thread-local */ public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) return (T)map.get(this); // Maps are constructed lazily. if the map for this thread // doesn't exist, create it, with this ThreadLocal and its // initial value as its only entry. T value = initialValue(); createMap(t, value); return value; &#125; /** * Sets the current thread's copy of this thread-local variable * to the specified value. Many applications will have no need for * this functionality, relying solely on the &#123;@link #initialValue&#125; * method to set the values of thread-locals. * * @param value the value to be stored in the current threads' copy of * this thread-local. */ public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); &#125; /** * Get the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @return the map */ ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals; &#125; /** * Create the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @param firstValue value for the initial entry of the map * @param map the map to store. */ void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue); &#125; ....... /** * ThreadLocalMap is a customized hash map suitable only for * maintaining thread local values. No operations are exported * outside of the ThreadLocal class. The class is package private to * allow declaration of fields in class Thread. To help deal with * very large and long-lived usages, the hash table entries use * WeakReferences for keys. However, since reference queues are not * used, stale entries are guaranteed to be removed only when * the table starts running out of space. */ static class ThreadLocalMap &#123; ........ &#125;&#125; 从中很清晰的可以看出，多个线程拥有自己一份单独的ThreadLocalMap，共享变量对于每个线程都是单独的一份，因此不会造成线程的安全问题。 JDBC的ConnectionManager类就是以这种方式来实现数据库连接Connection对象线程隔离。 1234567891011121314151617181920212223242526272829import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;public class ConnectionManager &#123; private static ThreadLocal&lt;Connection&gt; connectionHolder = new ThreadLocal&lt;Connection&gt;() &#123; @Override protected Connection initialValue() &#123; Connection conn = null; try &#123; conn = DriverManager.getConnection( \"jdbc:mysql://localhost:3306/test\", \"username\", \"password\"); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return conn; &#125; &#125;; public static Connection getConnection() &#123; return connectionHolder.get(); &#125; public static void setConnection(Connection conn) &#123; connectionHolder.set(conn); &#125;&#125; 但是，有些情况ThreadLocal可能并不适用，例如存储大量数据的共享变量，或共享变量只能被创建一次时，就只能通过synchronized来实现了。 推荐阅读","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://yoursite.com/categories/JAVA/"}],"tags":[{"name":"ThreadLocal","slug":"ThreadLocal","permalink":"http://yoursite.com/tags/ThreadLocal/"}]},{"title":"Spring-cloud入门介绍","slug":"spring/spring-boot/spring-introduction","date":"2017-07-22T10:52:56.000Z","updated":"2017-08-07T15:53:31.568Z","comments":true,"path":"2017/07/22/spring/spring-boot/spring-introduction/","link":"","permalink":"http://yoursite.com/2017/07/22/spring/spring-boot/spring-introduction/","excerpt":"Spring-cloud入门介绍Spring Cloud官网 Spring Cloud中文网 一、Spring Cloud Netflix二、服务提供与调用","text":"Spring-cloud入门介绍Spring Cloud官网 Spring Cloud中文网 一、Spring Cloud Netflix二、服务提供与调用 三、熔断器Hystrix四、熔断监控Hystrix Dashboard和Turbine五、配置中心git示例","categories":[{"name":"spring-cloud","slug":"spring-cloud","permalink":"http://yoursite.com/categories/spring-cloud/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"},{"name":"spring-cloud","slug":"spring-cloud","permalink":"http://yoursite.com/tags/spring-cloud/"}]},{"title":"第一篇博客","slug":"My-first-post","date":"2017-07-16T03:17:11.000Z","updated":"2018-03-15T16:19:49.226Z","comments":true,"path":"2017/07/16/My-first-post/","link":"","permalink":"http://yoursite.com/2017/07/16/My-first-post/","excerpt":"为什么我要开始要写博客&nbsp;&nbsp;&nbsp;&nbsp;从15年11月份以来，这一年多在企业工作的日子里，我收获许多。作为一个渴望学技术的程序员，我慢慢摆脱了学校的那种安逸的生活，开始走上了技术宅的道路。 &nbsp;&nbsp;&nbsp;&nbsp;在企业中，前几个月的时间里，我每天都像是海绵一样吸收着养分，学习企业的架构，项目的开发，部署，优化以及维护工作。我每天都痛苦并快乐着，虽然加班，但是我能感觉到自己一点一点的在往上爬。我学会SpringMVC架构，学会使用Maven构建项目，用Ant来实现自动部署项目，用Groovy脚本来编写告警任务。学会了很多软件，诸如MongoDB，Redis等常用开发软件。这个过程中，我很快乐，并且每天都在进步。","text":"为什么我要开始要写博客&nbsp;&nbsp;&nbsp;&nbsp;从15年11月份以来，这一年多在企业工作的日子里，我收获许多。作为一个渴望学技术的程序员，我慢慢摆脱了学校的那种安逸的生活，开始走上了技术宅的道路。 &nbsp;&nbsp;&nbsp;&nbsp;在企业中，前几个月的时间里，我每天都像是海绵一样吸收着养分，学习企业的架构，项目的开发，部署，优化以及维护工作。我每天都痛苦并快乐着，虽然加班，但是我能感觉到自己一点一点的在往上爬。我学会SpringMVC架构，学会使用Maven构建项目，用Ant来实现自动部署项目，用Groovy脚本来编写告警任务。学会了很多软件，诸如MongoDB，Redis等常用开发软件。这个过程中，我很快乐，并且每天都在进步。&nbsp;&nbsp;&nbsp;&nbsp;但是到了17年的3,4月份，我熟悉了团队的各种业务，也明白了项目中所用到的框架和各种技术。我每天做的除了日常的开发和维护，似乎陷入了重复造轮子的困境。虽然在这过程中，我学会了怎样去考虑到新业务或新场景的设计流程和后续的维护过程，但是我始终感觉到了自己的进步慢慢的缓下了，这是我不希望看到的，我渴望进步和成功。 &nbsp;&nbsp;&nbsp;&nbsp;最终，我看到了一句话，“种一棵树，最好的时间是十年前，其次，是现在”，我开始领悟到我必须改变点什么。我开始看基础的JAVA进阶等书籍，开始每天看技术博客或推文，培养自己的兴趣，并且从现在开始，写博客。我以前似乎总在犹豫，我常常害怕自己的技术不够好，但是，从另外一个方面想，这又有什么关系，那就学吧。 &nbsp;&nbsp;&nbsp;&nbsp;为什么写博客，因为它是对你看到，用到知识的升华。在写博客的过程中，不仅会让你review以前的代码，考虑更好的设计方案，也会让你对知识的理解更上一层楼，并记忆深刻。 &nbsp;&nbsp;&nbsp;&nbsp;写代码的时候，往往避免不了遇到各种BUG和技术难点，但是没关系，多请教别人，大多数人愿意和你分享自己的所得。和优秀的人多交流，你们会相互收益。最后，最重要的是，告诉自己不要怕，并且时刻保持一个谦卑的心。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]}]}